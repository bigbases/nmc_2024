{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 싱글모델로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import argparse\n",
    "import yaml\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "from torch import distributed as dist\n",
    "from nmc.models import *\n",
    "from nmc.datasets import * \n",
    "from nmc.augmentations import get_train_augmentation, get_val_augmentation\n",
    "from nmc.losses import get_loss\n",
    "from nmc.schedulers import get_scheduler\n",
    "from nmc.optimizers import get_optimizer\n",
    "from nmc.utils.utils import fix_seeds, setup_cudnn, cleanup_ddp, setup_ddp\n",
    "from tools.val import evaluate_epi\n",
    "from nmc.utils.episodic_utils import * \n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.cluster import hierarchy\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DEVICE': 'cuda:0', 'SAVE_DIR': 'output', 'MODEL': {'NAME': 'EfficientNetV2MModel', 'BACKBONE': 'EfficientNetV2', 'PRETRAINED': '/workspace/jhmoon/nmc_2024/checkpoints/pretrained/tf_efficientnetv2_m_weights.pth', 'UNFREEZE': 'full', 'VERSION': '384_32'}, 'DATASET': {'NAME': 'APTOSDataset', 'ROOT': '/data/public_data/aptos', 'TRAIN_RATIO': 0.7, 'VALID_RATIO': 0.15, 'TEST_RATIO': 0.15}, 'TRAIN': {'IMAGE_SIZE': [384, 384], 'BATCH_SIZE': 32, 'EPOCHS': 100, 'EVAL_INTERVAL': 25, 'AMP': False, 'DDP': False}, 'LOSS': {'NAME': 'CrossEntropy', 'CLS_WEIGHTS': False}, 'OPTIMIZER': {'NAME': 'adamw', 'LR': 0.001, 'WEIGHT_DECAY': 0.01}, 'SCHEDULER': {'NAME': 'warmuppolylr', 'POWER': 0.9, 'WARMUP': 10, 'WARMUP_RATIO': 0.1}, 'EVAL': {'MODEL_PATH': 'checkpoints/pretrained/FGMaxxVit/FGMaxxVit.FGMaxxVit.APTOS.pth', 'IMAGE_SIZE': [384, 384]}, 'TEST': {'MODEL_PATH': 'checkpoints/pretrained/FGMaxxVit/FGMaxxVit.FGMaxxVit.APTOS.pth', 'FILE': 'assests/ade', 'IMAGE_SIZE': [384, 384], 'OVERLAY': True}}\n"
     ]
    }
   ],
   "source": [
    "with open('../configs/APTOS.yaml') as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "print(cfg)\n",
    "fix_seeds(3407)\n",
    "setup_cudnn()\n",
    "gpu = setup_ddp()\n",
    "save_dir = Path(cfg['SAVE_DIR'])\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "cleanup_ddp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_augmentation(size):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.Lambda(lambda x: x.float() if x.dtype == torch.uint8 else x),\n",
    "        transforms.Lambda(lambda x: x / 255.0 if x.max() > 1.0 else x),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_val_test_transform(size):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.Lambda(lambda x: x.float() if x.dtype == torch.uint8 else x),\n",
    "        transforms.Lambda(lambda x: x / 255.0 if x.max() > 1.0 else x),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTargetBalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, target_classes):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.target_classes = target_classes\n",
    "        \n",
    "        # 데이터셋에서 레이블 추출\n",
    "        if hasattr(dataset, 'labels'):\n",
    "            self.labels = dataset.labels\n",
    "            if isinstance(self.labels, np.ndarray):\n",
    "                self.labels = torch.from_numpy(self.labels)\n",
    "        elif hasattr(dataset, 'targets'):\n",
    "            self.labels = dataset.targets\n",
    "            if isinstance(self.labels, np.ndarray):\n",
    "                self.labels = torch.from_numpy(self.labels)\n",
    "        else:\n",
    "            try:\n",
    "                self.labels = [sample[1] for sample in dataset]\n",
    "                if isinstance(self.labels[0], np.ndarray):\n",
    "                    self.labels = torch.from_numpy(np.array(self.labels))\n",
    "                else:\n",
    "                    self.labels = torch.tensor(self.labels)\n",
    "            except:\n",
    "                raise ValueError(\"Cannot access labels from dataset\")\n",
    "        \n",
    "        # 각 타겟 클래스와 나머지 클래스의 인덱스 저장\n",
    "        self.target_indices = {}\n",
    "        for target in target_classes:\n",
    "            if len(self.labels.shape) > 1:\n",
    "                self.target_indices[target] = torch.where(self.labels[:, target] == 1)[0]\n",
    "            else:\n",
    "                self.target_indices[target] = torch.where(self.labels == target)[0]\n",
    "        \n",
    "        # 나머지 클래스의 인덱스 저장\n",
    "        if len(self.labels.shape) > 1:\n",
    "            self.other_indices = torch.where(\n",
    "                torch.sum(self.labels[:, target_classes], dim=1) == 0)[0]\n",
    "        else:\n",
    "            mask = torch.ones_like(self.labels, dtype=torch.bool)\n",
    "            for target in target_classes:\n",
    "                mask &= (self.labels != target)\n",
    "            self.other_indices = torch.where(mask)[0]\n",
    "        \n",
    "        # 각 그룹당 샘플 수 계산\n",
    "        n_groups = len(target_classes) + 1  # 타겟 클래스들 + 나머지\n",
    "        self.samples_per_group = batch_size // n_groups\n",
    "        \n",
    "        self.n_batches = len(self.dataset) // batch_size\n",
    "        if len(self.dataset) % batch_size != 0:\n",
    "            self.n_batches += 1\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for _ in range(self.n_batches):\n",
    "            batch_indices = []\n",
    "            \n",
    "            # 각 타겟 클래스에서 샘플링\n",
    "            for target in self.target_classes:\n",
    "                target_selected = self.target_indices[target][\n",
    "                    torch.randint(len(self.target_indices[target]), \n",
    "                                (self.samples_per_group,))\n",
    "                ]\n",
    "                batch_indices.extend(target_selected.tolist())\n",
    "            \n",
    "            # 나머지 클래스들에서 샘플링\n",
    "            other_selected = self.other_indices[\n",
    "                torch.randint(len(self.other_indices), \n",
    "                            (self.samples_per_group,))\n",
    "            ]\n",
    "            batch_indices.extend(other_selected.tolist())\n",
    "            \n",
    "            # 배치 셔플\n",
    "            random.shuffle(batch_indices)\n",
    "            \n",
    "            # 배치 크기에 맞게 자르기 (나누어 떨어지지 않는 경우 처리)\n",
    "            if len(batch_indices) > self.batch_size:\n",
    "                batch_indices = batch_indices[:self.batch_size]\n",
    "            \n",
    "            yield batch_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda:1\n",
      "/data/public_data/aptos/combined_images\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "best_mf1 = 0.0\n",
    "device = torch.device(cfg['DEVICE'])\n",
    "device = \"cuda:1\"\n",
    "print(\"device : \", device)\n",
    "num_workers = mp.cpu_count()\n",
    "train_cfg, eval_cfg = cfg['TRAIN'], cfg['EVAL']\n",
    "dataset_cfg, model_cfg = cfg['DATASET'], cfg['MODEL']\n",
    "loss_cfg, optim_cfg, sched_cfg = cfg['LOSS'], cfg['OPTIMIZER'], cfg['SCHEDULER']\n",
    "epochs, lr = train_cfg['EPOCHS'], optim_cfg['LR']\n",
    "\n",
    "image_size = [256,256]\n",
    "image_dir = Path(dataset_cfg['ROOT']) / 'train_images'\n",
    "train_transform = get_train_augmentation(image_size)\n",
    "val_test_transform = get_val_test_transform(image_size)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "dataset = eval(dataset_cfg['NAME']+'Test')(\n",
    "    dataset_cfg['ROOT'] + '/combined_images',\n",
    "    transform=None,\n",
    "    target_label=None,\n",
    ")\n",
    "dataset.transform = val_test_transform\n",
    "# trainset, valset, testset = dataset.get_splits()\n",
    "# valset.transform = val_test_transform\n",
    "# testset.transform = val_test_transform\n",
    "\n",
    "# trainloader = DataLoader(trainset, batch_size=batch_size, num_workers=num_workers, drop_last=True, pin_memory=True)\n",
    "# valloader = DataLoader(valset, batch_size=1, num_workers=1, pin_memory=True)\n",
    "testloader = DataLoader(dataset, batch_size=1, num_workers=1, pin_memory=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import os\n",
    "def denormalize(tensor):\n",
    "   \"\"\"Denormalize the image tensor\"\"\"\n",
    "   mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "   std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "   return tensor * std + mean\n",
    "\n",
    "def get_gradcam(model, image, target_label_idx, device):\n",
    "    target_layer = model.features[-1]\n",
    "    cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "    \n",
    "    # target_label_idx가 리스트인 경우 첫 번째 값만 사용\n",
    "    target = ClassifierOutputTarget(0)  # 단일 클래스만 타겟팅\n",
    "    \n",
    "    grayscale_cam = cam(input_tensor=image.unsqueeze(0),\n",
    "                       targets=[target])\n",
    "    \n",
    "    return grayscale_cam[0]\n",
    "\n",
    "def create_comparison_image(original_img, nmc_heatmap, aptos_heatmap, label_info, nmc_correct, aptos_correct, save_path):\n",
    "   # Denormalize the original image\n",
    "   orig_img = denormalize(torch.from_numpy(original_img)).numpy()\n",
    "   orig_img = np.clip(orig_img.transpose(1, 2, 0), 0, 1)\n",
    "   orig_img = np.uint8(orig_img * 255)\n",
    "   \n",
    "   # Create heatmap overlays\n",
    "   nmc_heatmap_rgb = np.uint8(255 * nmc_heatmap)\n",
    "   nmc_heatmap_rgb = cv2.applyColorMap(nmc_heatmap_rgb, cv2.COLORMAP_JET)\n",
    "   nmc_superimposed = cv2.addWeighted(orig_img, 0.6, nmc_heatmap_rgb, 0.4, 0)\n",
    "   \n",
    "   aptos_heatmap_rgb = np.uint8(255 * aptos_heatmap)\n",
    "   aptos_heatmap_rgb = cv2.applyColorMap(aptos_heatmap_rgb, cv2.COLORMAP_JET)\n",
    "   aptos_superimposed = cv2.addWeighted(orig_img, 0.6, aptos_heatmap_rgb, 0.4, 0)\n",
    "   \n",
    "   # Create a white background for label info\n",
    "   height, width = orig_img.shape[:2]\n",
    "   info_height = height // 2  # 라벨 정보 영역의 높이를 절반으로\n",
    "   info_bg = np.ones((info_height, width * 3, 3), dtype=np.uint8) * 255  # 3배 너비\n",
    "   \n",
    "   # Add text to info background\n",
    "   font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "   font_scale = 0.5  # 글씨 크기 축소\n",
    "   thickness = 1\n",
    "   color = (0, 0, 0)  # Black color\n",
    "   \n",
    "   # Add label information\n",
    "   y_offset = 20\n",
    "   for line in label_info:\n",
    "       text_size = cv2.getTextSize(line, font, font_scale, thickness)[0]\n",
    "       x = (info_bg.shape[1] - text_size[0]) // 2\n",
    "       y = y_offset\n",
    "       cv2.putText(info_bg, line, (x, y), font, font_scale, color, thickness)\n",
    "       y_offset += 20\n",
    "   \n",
    "   # Create result texts\n",
    "   nmc_result = \"NMC: Correct\" if nmc_correct else \"NMC: Incorrect\"\n",
    "   aptos_result = \"APTOS: Correct\" if aptos_correct else \"APTOS: Incorrect\"\n",
    "   \n",
    "   # Add model results with color coding\n",
    "   text_size = cv2.getTextSize(nmc_result, font, font_scale, thickness)[0]\n",
    "   x = width + (width - text_size[0]) // 2\n",
    "   y = height // 2 - 20\n",
    "   color = (0, 255, 0) if nmc_correct else (0, 0, 255)\n",
    "   cv2.putText(info_bg, nmc_result, (x, y), font, font_scale, color, thickness)\n",
    "   \n",
    "   text_size = cv2.getTextSize(aptos_result, font, font_scale, thickness)[0]\n",
    "   x = 2 * width + (width - text_size[0]) // 2\n",
    "   color = (0, 255, 0) if aptos_correct else (0, 0, 255)\n",
    "   cv2.putText(info_bg, aptos_result, (x, y), font, font_scale, color, thickness)\n",
    "   \n",
    "   # Concatenate images horizontally\n",
    "   images_row = np.concatenate([orig_img, nmc_superimposed, aptos_superimposed], axis=1)\n",
    "   \n",
    "   # Concatenate info and images vertically\n",
    "   final_image = np.concatenate([info_bg, images_row], axis=0)\n",
    "   \n",
    "   # Save the final image\n",
    "   cv2.imwrite(save_path, cv2.cvtColor(final_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "def compare_and_save_gradcam(nmc_model, aptos_model, dataloader, nmc_label_idx, aptos_label_idx, device, save_dir='grad_results'):\n",
    "    nmc_model.eval()\n",
    "    aptos_model.eval()\n",
    "    \n",
    "    both_correct = []\n",
    "    only_nmc_correct = []\n",
    "    only_aptos_correct = []\n",
    "    both_wrong = []\n",
    "    \n",
    "    save_path = f\"{save_dir}/comparison/label_{'-'.join(map(str, nmc_label_idx))}_vs_{aptos_label_idx}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nProcessing APTOS label {aptos_label_idx} with corresponding NMC labels {nmc_label_idx}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels,img_name) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # 현재 데이터의 라벨이 우리가 평가하려는 라벨(aptos_label_idx)과 같은 경우만 처리\n",
    "            if labels.item() != aptos_label_idx:\n",
    "                continue\n",
    "                \n",
    "            # APTOS model predictions (binary classification for current label)\n",
    "            aptos_outputs = aptos_model(images)\n",
    "            aptos_predictions = (torch.sigmoid(aptos_outputs) > 0.5).squeeze()\n",
    "            aptos_raw_preds = torch.sigmoid(aptos_outputs).squeeze()\n",
    "            \n",
    "            # NMC model predictions\n",
    "            nmc_outputs = nmc_model(images)\n",
    "            nmc_predictions = (torch.sigmoid(nmc_outputs) > 0.5)\n",
    "            nmc_raw_preds = torch.sigmoid(nmc_outputs)\n",
    "            \n",
    "            # Handle batch size 1 case\n",
    "            if len(images) == 1:\n",
    "                if not aptos_predictions.shape:  # 스칼라인 경우\n",
    "                    aptos_predictions = aptos_predictions.unsqueeze(0)\n",
    "                    aptos_raw_preds = aptos_raw_preds.unsqueeze(0)\n",
    "                if not nmc_predictions.shape:  # 스칼라인 경우\n",
    "                    nmc_predictions = nmc_predictions.unsqueeze(0)\n",
    "                    nmc_raw_preds = nmc_raw_preds.unsqueeze(0)\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                try:\n",
    "                    # APTOS의 예측이 1이면 맞는 것 (현재 라벨에 대한 이진 분류)\n",
    "                    aptos_correct = aptos_predictions[i].item() == 1\n",
    "                    \n",
    "                    # NMC의 경우 모든 해당 라벨에 대해 1이어야 맞는 것\n",
    "                    nmc_is_correct = torch.all(nmc_predictions[i] == 1)\n",
    "                    \n",
    "                    # 디버깅을 위한 출력\n",
    "                    print(f\"\\nSample {i} in batch {batch_idx}:\")\n",
    "                    print(f\"APTOS prediction: {aptos_raw_preds[i].item():.3f}, Correct: {aptos_correct}\")\n",
    "                    print(f\"NMC predictions: {nmc_raw_preds[i].cpu().numpy()}, Correct: {nmc_is_correct}\")\n",
    "                    \n",
    "                    sample_info = {\n",
    "                        'image': images[i],\n",
    "                        'aptos_target': 1,  # 현재 라벨에 대해서는 항상 1이 타겟\n",
    "                        'nmc_preds': nmc_raw_preds[i].cpu().numpy(),\n",
    "                        'aptos_pred': aptos_raw_preds[i].item(),\n",
    "                        'img_name': img_name[i]  # 이미지 파일명 추가\n",
    "                    }\n",
    "                    \n",
    "                    # Categorize samples\n",
    "                    if aptos_correct and nmc_is_correct and len(both_correct) < 3:\n",
    "                        print(\"Adding to both_correct\")\n",
    "                        both_correct.append(sample_info)\n",
    "                    elif not aptos_correct and nmc_is_correct and len(only_nmc_correct) < 3:\n",
    "                        print(\"Adding to only_nmc_correct\")\n",
    "                        only_nmc_correct.append(sample_info)\n",
    "                    elif aptos_correct and not nmc_is_correct and len(only_aptos_correct) < 3:\n",
    "                        print(\"Adding to only_aptos_correct\")\n",
    "                        only_aptos_correct.append(sample_info)\n",
    "                    elif not aptos_correct and not nmc_is_correct and len(both_wrong) < 3:\n",
    "                        print(\"Adding to both_wrong\")\n",
    "                        both_wrong.append(sample_info)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing sample {i} in batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # 각 카테고리별 현재 수집된 샘플 수 출력\n",
    "                print(f\"\\nCurrent samples collected:\")\n",
    "                print(f\"Both correct: {len(both_correct)}\")\n",
    "                print(f\"Only NMC correct: {len(only_nmc_correct)}\")\n",
    "                print(f\"Only APTOS correct: {len(only_aptos_correct)}\")\n",
    "                print(f\"Both wrong: {len(both_wrong)}\")\n",
    "                \n",
    "                if (len(both_correct) >= 3 and \n",
    "                    len(only_nmc_correct) >= 3 and \n",
    "                    len(only_aptos_correct) >= 3 and \n",
    "                    len(both_wrong) >= 3):\n",
    "                    break\n",
    "                    \n",
    "            if (len(both_correct) >= 3 and \n",
    "                len(only_nmc_correct) >= 3 and \n",
    "                len(only_aptos_correct) >= 3 and \n",
    "                len(both_wrong) >= 3):\n",
    "                break\n",
    "    \n",
    "    print(f\"\\nFinal samples collected:\")\n",
    "    print(f\"Both correct: {len(both_correct)}\")\n",
    "    print(f\"Only NMC correct: {len(only_nmc_correct)}\")\n",
    "    print(f\"Only APTOS correct: {len(only_aptos_correct)}\")\n",
    "    print(f\"Both wrong: {len(both_wrong)}\")\n",
    "    \n",
    "    # Save GradCAM visualizations for all categories\n",
    "    for category, samples, category_name in [\n",
    "        (both_correct, \"both_correct\", \"Both Correct\"),\n",
    "        (only_nmc_correct, \"only_nmc\", \"Only NMC Correct\"),\n",
    "        (only_aptos_correct, \"only_aptos\", \"Only APTOS Correct\"),\n",
    "        (both_wrong, \"both_wrong\", \"Both Wrong\")\n",
    "    ]:\n",
    "        for idx, sample in enumerate(category):\n",
    "            try:\n",
    "                # Generate GradCAM for both models\n",
    "                nmc_heatmap = get_gradcam(nmc_model, sample['image'], nmc_label_idx, device)\n",
    "                aptos_heatmap = get_gradcam(aptos_model, sample['image'], [0], device)  # APTOS는 단일 출력\n",
    "                \n",
    "                # Prepare label info\n",
    "                nmc_pred_str = np.array2string(sample['nmc_preds'], precision=3)\n",
    "                aptos_pred_str = f\"{sample['aptos_pred']:.3f}\"\n",
    "                aptos_target_str = f\"{int(sample['aptos_target'])}\"\n",
    "                \n",
    "                nmc_is_correct = np.all(sample['nmc_preds'] > 0.5)\n",
    "                aptos_is_correct = (sample['aptos_pred'] > 0.5) == sample['aptos_target']\n",
    "                \n",
    "                label_info = [\n",
    "                    f\"Category: {category_name}\",\n",
    "                    f\"Image: {sample['img_name']}\",  # 이미지 파일명 추가\n",
    "                    f\"NMC Labels: {nmc_label_idx}, APTOS Label: {aptos_label_idx}\",\n",
    "                    f\"APTOS - True: {aptos_target_str}, Pred: {aptos_pred_str}\",\n",
    "                    f\"NMC Predictions: {nmc_pred_str}\"\n",
    "                ]\n",
    "                \n",
    "                # Save combined result\n",
    "                save_name = os.path.join(save_path, f'{category_name.lower().replace(\" \", \"_\")}_{idx}.png')\n",
    "                create_comparison_image(\n",
    "                    sample['image'].cpu().numpy(),\n",
    "                    nmc_heatmap,\n",
    "                    aptos_heatmap,\n",
    "                    label_info,\n",
    "                    nmc_is_correct,\n",
    "                    aptos_is_correct,\n",
    "                    save_name\n",
    "                )\n",
    "                print(f\"Saved image: {save_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {category_name} sample {idx}: {e}\")\n",
    "                print(f\"Sample info: {sample}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing NMC label [0] and APTOS label 0\n",
      "\n",
      "Processing APTOS label 0 with corresponding NMC labels [0]\n",
      "\n",
      "Sample 0 in batch 0:\n",
      "APTOS prediction: 1.000, Correct: True\n",
      "NMC predictions: [0.7937736], Correct: True\n",
      "Adding to both_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 1\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "\n",
      "Sample 0 in batch 1:\n",
      "APTOS prediction: 1.000, Correct: True\n",
      "NMC predictions: [0.81145424], Correct: True\n",
      "Adding to both_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 2\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "\n",
      "Sample 0 in batch 2:\n",
      "APTOS prediction: 1.000, Correct: True\n",
      "NMC predictions: [0.9791905], Correct: True\n",
      "Adding to both_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 3\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "\n",
      "Sample 0 in batch 3:\n",
      "APTOS prediction: 1.000, Correct: True\n",
      "NMC predictions: [0.9932122], Correct: True\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 3\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "\n",
      "Sample 0 in batch 4:\n",
      "APTOS prediction: 1.000, Correct: True\n",
      "NMC predictions: [0.98075026], Correct: True\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 3\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "\n",
      "Sample 0 in batch 5:\n",
      "APTOS prediction: 1.000, Correct: True\n",
      "NMC predictions: [0.9035225], Correct: True\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 3\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "\n",
      "Final samples collected:\n",
      "Both correct: 3\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "Saved image: grad_label_results/comparison/label_0_vs_0/both_correct_0.png\n",
      "Saved image: grad_label_results/comparison/label_0_vs_0/both_correct_1.png\n",
      "Saved image: grad_label_results/comparison/label_0_vs_0/both_correct_2.png\n",
      "\n",
      "Processing NMC label [2] and APTOS label 1\n",
      "\n",
      "Processing APTOS label 1 with corresponding NMC labels [2]\n",
      "\n",
      "Sample 0 in batch 6:\n",
      "APTOS prediction: 0.999, Correct: True\n",
      "NMC predictions: [0.99993896], Correct: True\n",
      "Adding to both_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 1\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "\n",
      "Sample 0 in batch 7:\n",
      "APTOS prediction: 1.000, Correct: True\n",
      "NMC predictions: [6.3236614e-05], Correct: False\n",
      "Adding to only_aptos_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 1\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 1\n",
      "Both wrong: 0\n",
      "\n",
      "Sample 0 in batch 8:\n",
      "APTOS prediction: 0.009, Correct: False\n",
      "NMC predictions: [0.01133983], Correct: False\n",
      "Adding to both_wrong\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 1\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 1\n",
      "Both wrong: 1\n",
      "\n",
      "Sample 0 in batch 9:\n",
      "APTOS prediction: 0.966, Correct: True\n",
      "NMC predictions: [0.8923267], Correct: True\n",
      "Adding to both_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 2\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 1\n",
      "Both wrong: 1\n",
      "\n",
      "Final samples collected:\n",
      "Both correct: 2\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 1\n",
      "Both wrong: 1\n",
      "Saved image: grad_label_results/comparison/label_2_vs_1/both_correct_0.png\n",
      "Saved image: grad_label_results/comparison/label_2_vs_1/both_correct_1.png\n",
      "Saved image: grad_label_results/comparison/label_2_vs_1/only_aptos_correct_0.png\n",
      "Saved image: grad_label_results/comparison/label_2_vs_1/both_wrong_0.png\n",
      "\n",
      "Processing NMC label [1] and APTOS label 2\n",
      "\n",
      "Processing APTOS label 2 with corresponding NMC labels [1]\n",
      "\n",
      "Sample 0 in batch 10:\n",
      "APTOS prediction: 0.889, Correct: True\n",
      "NMC predictions: [0.99998116], Correct: True\n",
      "Adding to both_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 1\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "\n",
      "Sample 0 in batch 11:\n",
      "APTOS prediction: 0.801, Correct: True\n",
      "NMC predictions: [0.99991405], Correct: True\n",
      "Adding to both_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 2\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "\n",
      "Sample 0 in batch 12:\n",
      "APTOS prediction: 0.991, Correct: True\n",
      "NMC predictions: [0.01146417], Correct: False\n",
      "Adding to only_aptos_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 2\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 1\n",
      "Both wrong: 0\n",
      "\n",
      "Sample 0 in batch 13:\n",
      "APTOS prediction: 0.981, Correct: True\n",
      "NMC predictions: [0.9966882], Correct: True\n",
      "Adding to both_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 3\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 1\n",
      "Both wrong: 0\n",
      "\n",
      "Final samples collected:\n",
      "Both correct: 3\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 1\n",
      "Both wrong: 0\n",
      "Saved image: grad_label_results/comparison/label_1_vs_2/both_correct_0.png\n",
      "Saved image: grad_label_results/comparison/label_1_vs_2/both_correct_1.png\n",
      "Saved image: grad_label_results/comparison/label_1_vs_2/both_correct_2.png\n",
      "Saved image: grad_label_results/comparison/label_1_vs_2/only_aptos_correct_0.png\n",
      "\n",
      "Processing NMC label [1, 2] and APTOS label 3\n",
      "\n",
      "Processing APTOS label 3 with corresponding NMC labels [1, 2]\n",
      "\n",
      "Sample 0 in batch 14:\n",
      "APTOS prediction: 1.000, Correct: True\n",
      "NMC predictions: [0.999977   0.99998474], Correct: True\n",
      "Adding to both_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 1\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "\n",
      "Sample 0 in batch 15:\n",
      "APTOS prediction: 1.000, Correct: True\n",
      "NMC predictions: [0.9998596 0.9999782], Correct: True\n",
      "Adding to both_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 2\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "\n",
      "Sample 0 in batch 16:\n",
      "APTOS prediction: 0.000, Correct: False\n",
      "NMC predictions: [0.99999917 0.999987  ], Correct: True\n",
      "Adding to only_nmc_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 2\n",
      "Only NMC correct: 1\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "\n",
      "Final samples collected:\n",
      "Both correct: 2\n",
      "Only NMC correct: 1\n",
      "Only APTOS correct: 0\n",
      "Both wrong: 0\n",
      "Saved image: grad_label_results/comparison/label_1-2_vs_3/both_correct_0.png\n",
      "Saved image: grad_label_results/comparison/label_1-2_vs_3/both_correct_1.png\n",
      "Saved image: grad_label_results/comparison/label_1-2_vs_3/only_nmc_correct_0.png\n",
      "\n",
      "Processing NMC label [5, 6] and APTOS label 4\n",
      "\n",
      "Processing APTOS label 4 with corresponding NMC labels [5, 6]\n",
      "\n",
      "Sample 0 in batch 17:\n",
      "APTOS prediction: 1.000, Correct: True\n",
      "NMC predictions: [2.516379e-05 1.464620e-04], Correct: False\n",
      "Adding to only_aptos_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 0\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 1\n",
      "Both wrong: 0\n",
      "\n",
      "Sample 0 in batch 18:\n",
      "APTOS prediction: 0.000, Correct: False\n",
      "NMC predictions: [0.0004674  0.00178404], Correct: False\n",
      "Adding to both_wrong\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 0\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 1\n",
      "Both wrong: 1\n",
      "\n",
      "Sample 0 in batch 19:\n",
      "APTOS prediction: 0.088, Correct: False\n",
      "NMC predictions: [1.2001531e-05 7.6317048e-04], Correct: False\n",
      "Adding to both_wrong\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 0\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 1\n",
      "Both wrong: 2\n",
      "\n",
      "Sample 0 in batch 20:\n",
      "APTOS prediction: 1.000, Correct: True\n",
      "NMC predictions: [6.1094615e-06 4.2560068e-04], Correct: False\n",
      "Adding to only_aptos_correct\n",
      "\n",
      "Current samples collected:\n",
      "Both correct: 0\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 2\n",
      "Both wrong: 2\n",
      "\n",
      "Final samples collected:\n",
      "Both correct: 0\n",
      "Only NMC correct: 0\n",
      "Only APTOS correct: 2\n",
      "Both wrong: 2\n",
      "Saved image: grad_label_results/comparison/label_5-6_vs_4/only_aptos_correct_0.png\n",
      "Saved image: grad_label_results/comparison/label_5-6_vs_4/only_aptos_correct_1.png\n",
      "Saved image: grad_label_results/comparison/label_5-6_vs_4/both_wrong_0.png\n",
      "Saved image: grad_label_results/comparison/label_5-6_vs_4/both_wrong_1.png\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "nmc_labels = [[0],[2],[1],[1,2],[5,6]]\n",
    "aptos_labels = [0,1,2,3,4]  # 각각 대응되는 APTOS 라벨\n",
    "\n",
    "for idx, nmc_label_idx in enumerate(nmc_labels):\n",
    "   aptos_label_idx = aptos_labels[idx]\n",
    "   \n",
    "   print(f\"\\nProcessing NMC label {nmc_label_idx} and APTOS label {aptos_label_idx}\")\n",
    "   \n",
    "   # Load NMC model\n",
    "   nmc_model = models.efficientnet_v2_m(pretrained=True)\n",
    "   num_ftrs = nmc_model.classifier[1].in_features\n",
    "   nmc_model.classifier = nn.Sequential(\n",
    "       nn.BatchNorm1d(num_ftrs),\n",
    "       nn.Linear(num_ftrs, len(nmc_label_idx))\n",
    "   )\n",
    "   nmc_model = nmc_model.to(device)\n",
    "   \n",
    "   if len(nmc_label_idx)==1:\n",
    "       nmc_model.load_state_dict(torch.load(f'model/singlelabel/best_model_label_{nmc_label_idx[0]}_nmc_cnn.pth'))\n",
    "   else:\n",
    "       nmc_model.load_state_dict(torch.load(f'model/singlelabel/best_model_labels_{\"-\".join(map(str,nmc_label_idx))}_nmc_cnn.pth'))\n",
    "   \n",
    "   # Load APTOS model\n",
    "   aptos_model = models.efficientnet_v2_m(pretrained=True)\n",
    "   aptos_model.classifier = nn.Sequential(\n",
    "       nn.BatchNorm1d(num_ftrs),\n",
    "       nn.Linear(num_ftrs, 1)\n",
    "   )\n",
    "   aptos_model = aptos_model.to(device)\n",
    "   aptos_model.load_state_dict(torch.load(f'model/singlelabel/best_model_label_{aptos_label_idx}_aptos_cnn.pth'))\n",
    "   \n",
    "   # Compare and save results\n",
    "   compare_and_save_gradcam(nmc_model, aptos_model, testloader, nmc_label_idx, aptos_label_idx, device,save_dir='grad_label_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
