{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 싱글모델로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import argparse\n",
    "import yaml\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "from torch import distributed as dist\n",
    "from nmc.models import *\n",
    "from nmc.datasets import * \n",
    "from nmc.augmentations import get_train_augmentation, get_val_augmentation\n",
    "from nmc.losses import get_loss\n",
    "from nmc.schedulers import get_scheduler\n",
    "from nmc.optimizers import get_optimizer\n",
    "from nmc.utils.utils import fix_seeds, setup_cudnn, cleanup_ddp, setup_ddp\n",
    "from tools.val import evaluate_epi\n",
    "from nmc.utils.episodic_utils import * \n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.cluster import hierarchy\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DEVICE': 'cuda:0', 'SAVE_DIR': 'output', 'MODEL': {'NAME': 'EfficientNetV2MModel', 'BACKBONE': 'EfficientNetV2', 'PRETRAINED': '/workspace/jhmoon/nmc_2024/checkpoints/pretrained/tf_efficientnetv2_m_weights.pth', 'UNFREEZE': 'full', 'VERSION': '384_32'}, 'DATASET': {'NAME': 'APTOSDataset', 'ROOT': '/data/public_data/aptos', 'TRAIN_RATIO': 0.7, 'VALID_RATIO': 0.15, 'TEST_RATIO': 0.15}, 'TRAIN': {'IMAGE_SIZE': [384, 384], 'BATCH_SIZE': 32, 'EPOCHS': 100, 'EVAL_INTERVAL': 25, 'AMP': False, 'DDP': False}, 'LOSS': {'NAME': 'CrossEntropy', 'CLS_WEIGHTS': False}, 'OPTIMIZER': {'NAME': 'adamw', 'LR': 0.001, 'WEIGHT_DECAY': 0.01}, 'SCHEDULER': {'NAME': 'warmuppolylr', 'POWER': 0.9, 'WARMUP': 10, 'WARMUP_RATIO': 0.1}, 'EVAL': {'MODEL_PATH': 'checkpoints/pretrained/FGMaxxVit/FGMaxxVit.FGMaxxVit.APTOS.pth', 'IMAGE_SIZE': [384, 384]}, 'TEST': {'MODEL_PATH': 'checkpoints/pretrained/FGMaxxVit/FGMaxxVit.FGMaxxVit.APTOS.pth', 'FILE': 'assests/ade', 'IMAGE_SIZE': [384, 384], 'OVERLAY': True}}\n"
     ]
    }
   ],
   "source": [
    "with open('../configs/APTOS.yaml') as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "print(cfg)\n",
    "fix_seeds(3407)\n",
    "setup_cudnn()\n",
    "gpu = setup_ddp()\n",
    "save_dir = Path(cfg['SAVE_DIR'])\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "cleanup_ddp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_augmentation(size):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.Lambda(lambda x: x.float() if x.dtype == torch.uint8 else x),\n",
    "        transforms.Lambda(lambda x: x / 255.0 if x.max() > 1.0 else x),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_val_test_transform(size):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.Lambda(lambda x: x.float() if x.dtype == torch.uint8 else x),\n",
    "        transforms.Lambda(lambda x: x / 255.0 if x.max() > 1.0 else x),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTargetBalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, target_classes):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.target_classes = target_classes\n",
    "        \n",
    "        # 데이터셋에서 레이블 추출\n",
    "        if hasattr(dataset, 'labels'):\n",
    "            self.labels = dataset.labels\n",
    "            if isinstance(self.labels, np.ndarray):\n",
    "                self.labels = torch.from_numpy(self.labels)\n",
    "        elif hasattr(dataset, 'targets'):\n",
    "            self.labels = dataset.targets\n",
    "            if isinstance(self.labels, np.ndarray):\n",
    "                self.labels = torch.from_numpy(self.labels)\n",
    "        else:\n",
    "            try:\n",
    "                self.labels = [sample[1] for sample in dataset]\n",
    "                if isinstance(self.labels[0], np.ndarray):\n",
    "                    self.labels = torch.from_numpy(np.array(self.labels))\n",
    "                else:\n",
    "                    self.labels = torch.tensor(self.labels)\n",
    "            except:\n",
    "                raise ValueError(\"Cannot access labels from dataset\")\n",
    "        \n",
    "        # 각 타겟 클래스와 나머지 클래스의 인덱스 저장\n",
    "        self.target_indices = {}\n",
    "        for target in target_classes:\n",
    "            if len(self.labels.shape) > 1:\n",
    "                self.target_indices[target] = torch.where(self.labels[:, target] == 1)[0]\n",
    "            else:\n",
    "                self.target_indices[target] = torch.where(self.labels == target)[0]\n",
    "        \n",
    "        # 나머지 클래스의 인덱스 저장\n",
    "        if len(self.labels.shape) > 1:\n",
    "            self.other_indices = torch.where(\n",
    "                torch.sum(self.labels[:, target_classes], dim=1) == 0)[0]\n",
    "        else:\n",
    "            mask = torch.ones_like(self.labels, dtype=torch.bool)\n",
    "            for target in target_classes:\n",
    "                mask &= (self.labels != target)\n",
    "            self.other_indices = torch.where(mask)[0]\n",
    "        \n",
    "        # 각 그룹당 샘플 수 계산\n",
    "        n_groups = len(target_classes) + 1  # 타겟 클래스들 + 나머지\n",
    "        self.samples_per_group = batch_size // n_groups\n",
    "        \n",
    "        self.n_batches = len(self.dataset) // batch_size\n",
    "        if len(self.dataset) % batch_size != 0:\n",
    "            self.n_batches += 1\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for _ in range(self.n_batches):\n",
    "            batch_indices = []\n",
    "            \n",
    "            # 각 타겟 클래스에서 샘플링\n",
    "            for target in self.target_classes:\n",
    "                target_selected = self.target_indices[target][\n",
    "                    torch.randint(len(self.target_indices[target]), \n",
    "                                (self.samples_per_group,))\n",
    "                ]\n",
    "                batch_indices.extend(target_selected.tolist())\n",
    "            \n",
    "            # 나머지 클래스들에서 샘플링\n",
    "            other_selected = self.other_indices[\n",
    "                torch.randint(len(self.other_indices), \n",
    "                            (self.samples_per_group,))\n",
    "            ]\n",
    "            batch_indices.extend(other_selected.tolist())\n",
    "            \n",
    "            # 배치 셔플\n",
    "            random.shuffle(batch_indices)\n",
    "            \n",
    "            # 배치 크기에 맞게 자르기 (나누어 떨어지지 않는 경우 처리)\n",
    "            if len(batch_indices) > self.batch_size:\n",
    "                batch_indices = batch_indices[:self.batch_size]\n",
    "            \n",
    "            yield batch_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda:1\n",
      "/data/public_data/aptos/combined_images\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "best_mf1 = 0.0\n",
    "device = torch.device(cfg['DEVICE'])\n",
    "device = \"cuda:1\"\n",
    "print(\"device : \", device)\n",
    "num_workers = mp.cpu_count()\n",
    "train_cfg, eval_cfg = cfg['TRAIN'], cfg['EVAL']\n",
    "dataset_cfg, model_cfg = cfg['DATASET'], cfg['MODEL']\n",
    "loss_cfg, optim_cfg, sched_cfg = cfg['LOSS'], cfg['OPTIMIZER'], cfg['SCHEDULER']\n",
    "epochs, lr = train_cfg['EPOCHS'], optim_cfg['LR']\n",
    "\n",
    "image_size = [256,256]\n",
    "image_dir = Path(dataset_cfg['ROOT']) / 'train_images'\n",
    "train_transform = get_train_augmentation(image_size)\n",
    "val_test_transform = get_val_test_transform(image_size)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "dataset = eval(dataset_cfg['NAME']+'Test')(\n",
    "    dataset_cfg['ROOT'] + '/combined_images',\n",
    "    transform=None,\n",
    "    target_label=None,\n",
    ")\n",
    "dataset.transform = val_test_transform\n",
    "# trainset, valset, testset = dataset.get_splits()\n",
    "# valset.transform = val_test_transform\n",
    "# testset.transform = val_test_transform\n",
    "\n",
    "# trainloader = DataLoader(trainset, batch_size=batch_size, num_workers=num_workers, drop_last=True, pin_memory=True)\n",
    "# valloader = DataLoader(valset, batch_size=1, num_workers=1, pin_memory=True)\n",
    "testloader = DataLoader(dataset, batch_size=1, num_workers=1, pin_memory=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import os\n",
    "from captum.attr import IntegratedGradients, LayerLRP\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "def denormalize(tensor):\n",
    "   \"\"\"Denormalize the image tensor\"\"\"\n",
    "   mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "   std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "   return tensor * std + mean\n",
    "\n",
    "def get_gradcam(model, image, target_label_idx, device):\n",
    "   target_layer = model.features[-1]\n",
    "   cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "   \n",
    "   # target_label_idx가 리스트인 경우 첫 번째 값만 사용\n",
    "   target = ClassifierOutputTarget(0)  # 단일 클래스만 타겟팅\n",
    "   \n",
    "   grayscale_cam = cam(input_tensor=image.unsqueeze(0),\n",
    "                      targets=[target])\n",
    "   \n",
    "   return grayscale_cam[0]\n",
    "\n",
    "def get_integrated_gradients(model, image, target_label_idx, device):\n",
    "    ig = IntegratedGradients(model)\n",
    "    \n",
    "    image = image.clone().detach().requires_grad_(True).to(device)\n",
    "    image = image.unsqueeze(0)\n",
    "    baseline = torch.zeros_like(image).to(device)\n",
    "    \n",
    "    try:\n",
    "        attributions = ig.attribute(\n",
    "            image,\n",
    "            baseline,\n",
    "            target=0,\n",
    "            n_steps=50\n",
    "        )\n",
    "        \n",
    "        attribution_map = attributions.squeeze().permute(1, 2, 0).cpu().detach().numpy()\n",
    "        denorm_image = denormalize(image.squeeze().cpu().detach()).permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # 원본 이미지를 0-255 범위의 uint8로 변환 후 흑백으로 변환\n",
    "        orig_uint8 = np.uint8(denorm_image * 255)\n",
    "        gray_image = cv2.cvtColor(orig_uint8, cv2.COLOR_RGB2GRAY)\n",
    "        gray_image = cv2.cvtColor(gray_image, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        visualization = visualize(\n",
    "            attributions=attribution_map,\n",
    "            image=gray_image,  # uint8 형식의 흑백 이미지 사용\n",
    "            positive_channel=[255, 0, 0],\n",
    "            negative_channel=[0, 0, 0],\n",
    "            polarity='positive',\n",
    "            clip_above_percentile=99,\n",
    "            clip_below_percentile=70,\n",
    "            overlay=True,\n",
    "            mask_mode=False  # overlay 모드 사용\n",
    "        )\n",
    "        \n",
    "        # 최종 결과를 0-1 범위로 정규화\n",
    "        visualization = visualization / 255.0\n",
    "        \n",
    "        return visualization\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Integrated Gradients: {e}\")\n",
    "        return np.zeros((image.shape[2], image.shape[3], 3))\n",
    "\n",
    "def get_lrp(model, image, target_label_idx, device):\n",
    "    # 모델의 마지막 feature layer를 타겟으로 설정\n",
    "    target_layer = model.features[-1]\n",
    "    lrp = LayerLRP(model, target_layer)\n",
    "    \n",
    "    # requires_grad 설정 및 차원 추가\n",
    "    image = image.clone().detach().requires_grad_(True)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    try:\n",
    "        attributions = lrp.attribute(\n",
    "            image,  # 이미 batch dimension이 추가된 상태\n",
    "            target=0\n",
    "        )\n",
    "        \n",
    "        # 속성값을 시각화 가능한 형태로 변환\n",
    "        attribution_map = torch.sum(torch.abs(attributions), dim=1).squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        # 정규화\n",
    "        if attribution_map.max() != attribution_map.min():\n",
    "            attribution_map = (attribution_map - attribution_map.min()) / (attribution_map.max() - attribution_map.min())\n",
    "        else:\n",
    "            attribution_map = np.zeros_like(attribution_map)\n",
    "        \n",
    "        return attribution_map\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LRP: {e}\")\n",
    "        # 에러 발생 시 zero map 반환\n",
    "        return np.zeros((image.shape[2], image.shape[3]))\n",
    "    \n",
    "def create_visualization_comparison(original_img, methods_results, label_info, is_correct, save_path):\n",
    "    # Denormalize original image\n",
    "    orig_img = denormalize(torch.from_numpy(original_img)).numpy()\n",
    "    orig_img = np.clip(orig_img.transpose(1, 2, 0), 0, 1)\n",
    "    orig_img = np.uint8(orig_img * 255)\n",
    "\n",
    "    # Calculate dimensions\n",
    "    height, width = orig_img.shape[:2]\n",
    "    info_height = height // 3\n",
    "\n",
    "    # Get original image name and find corresponding label image\n",
    "    img_name = [line for line in label_info if \"Image: \" in line][0].split(\"Image: \")[1]\n",
    "    img_base_name = img_name.split('.')[0]\n",
    "    label_img_path = find_label_image(img_base_name)\n",
    "\n",
    "    # Load and resize label image\n",
    "    try:\n",
    "        if label_img_path:\n",
    "            print(f\"Found label image: {label_img_path}\")\n",
    "            label_img = cv2.imread(label_img_path)\n",
    "            if label_img is not None:\n",
    "                label_img = cv2.cvtColor(label_img, cv2.COLOR_BGR2RGB)\n",
    "                label_img = cv2.resize(label_img, (width, height))\n",
    "            else:\n",
    "                label_img = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "        else:\n",
    "            label_img = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "    except Exception as e:\n",
    "        label_img = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "        \n",
    "    # Create info background\n",
    "    info_bg = np.ones((info_height, width * 5, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    # Define text parameters\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.5\n",
    "    thickness = 1\n",
    "    color = (0, 0, 0)\n",
    "\n",
    "    # Add text to info background\n",
    "    y_offset = 20\n",
    "    for line in label_info:\n",
    "        text_size = cv2.getTextSize(line, font, font_scale, thickness)[0]\n",
    "        x = (info_bg.shape[1] - text_size[0]) // 2\n",
    "        cv2.putText(info_bg, line, (x, y_offset), font, font_scale, color, thickness)\n",
    "        y_offset += 20\n",
    "    \n",
    "    # Create image labels\n",
    "    image_labels = ['Original with Label', 'GradCAM (NMC)', 'GradCAM (APTOS)', 'IG (NMC)', 'IG (APTOS)']\n",
    "    label_bg = np.ones((30, width * 5, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    for idx, label in enumerate(image_labels):\n",
    "        text_size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "        x = width * idx + (width - text_size[0]) // 2\n",
    "        cv2.putText(label_bg, label, (x, 20), font, font_scale, color, thickness)\n",
    "\n",
    "    # 원본 이미지를 흑백으로 변환\n",
    "    orig_img_gray = cv2.cvtColor(orig_img, cv2.COLOR_RGB2GRAY)\n",
    "    orig_img_gray = cv2.cvtColor(orig_img_gray, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # GradCAM 처리 수정\n",
    "    gradcam_nmc_raw = methods_results['GradCAM']['nmc']\n",
    "    gradcam_aptos_raw = methods_results['GradCAM']['aptos']\n",
    "    \n",
    "    # threshold 적용\n",
    "    threshold = 0.7  # 70% threshold\n",
    "    \n",
    "    # GradCAM용 시각화 함수 적용\n",
    "    def visualize_gradcam(cam_output, original_image_gray):\n",
    "        # threshold 적용\n",
    "        cam_output[cam_output < threshold] = 0\n",
    "        \n",
    "        # 빨간색 시각화 생성 (RGB)\n",
    "        heatmap = np.zeros((cam_output.shape[0], cam_output.shape[1], 3), dtype=np.uint8)\n",
    "        heatmap[..., 0] = np.uint8(255 * cam_output)  # Red channel\n",
    "        \n",
    "        # overlay with grayscale image\n",
    "        output = cv2.addWeighted(original_image_gray, 0.7, heatmap, 0.3, 0)\n",
    "        return output\n",
    "\n",
    "    # GradCAM 결과 시각화\n",
    "    gradcam_nmc = visualize_gradcam(gradcam_nmc_raw, orig_img_gray)\n",
    "    gradcam_aptos = visualize_gradcam(gradcam_aptos_raw, orig_img_gray)\n",
    "    \n",
    "    # Integrated Gradients - get_integrated_gradients 함수도 수정 필요\n",
    "    ig_nmc = np.uint8(255 * methods_results['Integrated Gradients']['nmc'])\n",
    "    ig_aptos = np.uint8(255 * methods_results['Integrated Gradients']['aptos'])\n",
    "    \n",
    "    # images_row 생성\n",
    "    images_row = np.concatenate([\n",
    "        label_img,\n",
    "        gradcam_nmc, \n",
    "        gradcam_aptos, \n",
    "        ig_nmc, \n",
    "        ig_aptos\n",
    "    ], axis=1)\n",
    "\n",
    "    # Combine all elements vertically\n",
    "    final_image = np.vstack([info_bg, label_bg, images_row])\n",
    "\n",
    "    # Save the final image\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(final_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "def find_label_image(base_name, label_dir='nmc_labeling'):\n",
    "    \"\"\"Find label image that contains the base image name\"\"\"\n",
    "    try:\n",
    "        for filename in os.listdir(label_dir):\n",
    "            if base_name in filename:\n",
    "                return os.path.join(label_dir, filename)\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def compare_and_save_visualizations(nmc_model, aptos_model, dataloader, nmc_label_idx, aptos_label_idx, device, save_dir='visualization_results'):\n",
    "    nmc_model.eval()\n",
    "    aptos_model.eval()\n",
    "    \n",
    "    both_correct = []\n",
    "    only_nmc_correct = []\n",
    "    only_aptos_correct = []\n",
    "    both_wrong = []\n",
    "    \n",
    "    save_path = f\"{save_dir}/comparison/label_{'-'.join(map(str, nmc_label_idx))}_vs_{aptos_label_idx}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nProcessing APTOS label {aptos_label_idx} with corresponding NMC labels {nmc_label_idx}\")\n",
    "    \n",
    "    for batch_idx, (images, labels, img_name) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        if labels.item() != aptos_label_idx:\n",
    "            continue\n",
    "        \n",
    "        # APTOS model predictions\n",
    "        aptos_outputs = aptos_model(images)\n",
    "        aptos_predictions = (torch.sigmoid(aptos_outputs) > 0.5).squeeze()\n",
    "        aptos_raw_preds = torch.sigmoid(aptos_outputs).squeeze()\n",
    "        \n",
    "        # NMC model predictions\n",
    "        nmc_outputs = nmc_model(images)\n",
    "        nmc_predictions = (torch.sigmoid(nmc_outputs) > 0.5)\n",
    "        nmc_raw_preds = torch.sigmoid(nmc_outputs)\n",
    "        \n",
    "        # Handle batch size 1 case\n",
    "        if len(images) == 1:\n",
    "            if not aptos_predictions.shape:\n",
    "                aptos_predictions = aptos_predictions.unsqueeze(0)\n",
    "                aptos_raw_preds = aptos_raw_preds.unsqueeze(0)\n",
    "            if not nmc_predictions.shape:\n",
    "                nmc_predictions = nmc_predictions.unsqueeze(0)\n",
    "                nmc_raw_preds = nmc_raw_preds.unsqueeze(0)\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            try:\n",
    "                # 예측 결과 확인\n",
    "                aptos_correct = aptos_predictions[i].item() == 1\n",
    "                nmc_is_correct = torch.all(nmc_predictions[i] == 1)\n",
    "                \n",
    "                # Generate visualizations for each method\n",
    "                methods_results = {\n",
    "                    'GradCAM': {\n",
    "                        'nmc': get_gradcam(nmc_model, images[i], nmc_label_idx, device),\n",
    "                        'aptos': get_gradcam(aptos_model, images[i], [0], device)\n",
    "                    },\n",
    "                    'Integrated Gradients': {\n",
    "                        'nmc': get_integrated_gradients(nmc_model, images[i], nmc_label_idx, device),\n",
    "                        'aptos': get_integrated_gradients(aptos_model, images[i], [0], device)\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Determine category\n",
    "                if aptos_correct and nmc_is_correct:\n",
    "                    category = 'both_correct'\n",
    "                elif nmc_is_correct and not aptos_correct:\n",
    "                    category = 'only_nmc_correct'\n",
    "                elif aptos_correct and not nmc_is_correct:\n",
    "                    category = 'only_aptos_correct'\n",
    "                else:\n",
    "                    category = 'both_wrong'\n",
    "                \n",
    "                if len(eval(category)) < 3:\n",
    "                    sample_info = {\n",
    "                        'image': images[i].detach().cpu(),  # detach 추가\n",
    "                        'methods_results': methods_results,\n",
    "                        'label_info': [\n",
    "                            f\"Category: {category}\",\n",
    "                            f\"Image: {img_name[i]}\",\n",
    "                            f\"NMC Labels: {nmc_label_idx}, APTOS Label: {aptos_label_idx}\",\n",
    "                            f\"APTOS Prediction: {aptos_raw_preds[i].item():.3f}\",\n",
    "                            f\"NMC Predictions: {nmc_raw_preds[i].detach().cpu().numpy()}\"  # detach 추가\n",
    "                        ],\n",
    "                        'is_correct': {\n",
    "                            'nmc': nmc_is_correct,\n",
    "                            'aptos': aptos_correct\n",
    "                        }\n",
    "                    }\n",
    "                    eval(category).append(sample_info)\n",
    "                \n",
    "                if (len(both_correct) >= 3 and \n",
    "                    len(only_nmc_correct) >= 3 and \n",
    "                    len(only_aptos_correct) >= 3 and \n",
    "                    len(both_wrong) >= 3):\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i} in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        if (len(both_correct) >= 3 and \n",
    "            len(only_nmc_correct) >= 3 and \n",
    "            len(only_aptos_correct) >= 3 and \n",
    "            len(both_wrong) >= 3):\n",
    "            break\n",
    "    \n",
    "    # Save visualizations for all categories\n",
    "    for category, samples, category_name in [\n",
    "        (both_correct, \"both_correct\", \"Both Correct\"),\n",
    "        (only_nmc_correct, \"only_nmc\", \"Only NMC Correct\"),\n",
    "        (only_aptos_correct, \"only_aptos\", \"Only APTOS Correct\"),\n",
    "        (both_wrong, \"both_wrong\", \"Both Wrong\")\n",
    "    ]:\n",
    "        for idx, sample in enumerate(category):\n",
    "            try:\n",
    "                save_name = os.path.join(save_path, f'{category_name.lower().replace(\" \", \"_\")}_{idx}.png')\n",
    "                create_visualization_comparison(\n",
    "                    sample['image'].numpy(),\n",
    "                    sample['methods_results'],\n",
    "                    sample['label_info'],\n",
    "                    sample['is_correct'],\n",
    "                    save_name\n",
    "                )\n",
    "                print(f\"Saved visualization: {save_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error creating visualization for {category_name} sample {idx}: {e}\")\n",
    "\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = [0, 255, 0]\n",
    "R = [255, 0, 0]\n",
    "\n",
    "def convert_to_gray_scale(attributions):\n",
    "    return np.average(attributions, axis=2)\n",
    "\n",
    "def linear_transform(attributions, clip_above_percentile=99.9, clip_below_percentile=70.0, low=0.2, plot_distribution=False):\n",
    "    m = compute_threshold_by_top_percentage(attributions, percentage=100-clip_above_percentile, plot_distribution=plot_distribution)\n",
    "    e = compute_threshold_by_top_percentage(attributions, percentage=100-clip_below_percentile, plot_distribution=plot_distribution)\n",
    "    transformed = (1 - low) * (np.abs(attributions) - e) / (m - e) + low\n",
    "    transformed *= np.sign(attributions)\n",
    "    transformed *= (transformed >= low)\n",
    "    transformed = np.clip(transformed, 0.0, 1.0)\n",
    "    return transformed\n",
    "\n",
    "def compute_threshold_by_top_percentage(attributions, percentage=60, plot_distribution=True):\n",
    "    if percentage < 0 or percentage > 100:\n",
    "        raise ValueError('percentage must be in [0, 100]')\n",
    "    if percentage == 100:\n",
    "        return np.min(attributions)\n",
    "    flat_attributions = attributions.flatten()\n",
    "    attribution_sum = np.sum(flat_attributions)\n",
    "    sorted_attributions = np.sort(np.abs(flat_attributions))[::-1]\n",
    "    cum_sum = 100.0 * np.cumsum(sorted_attributions) / attribution_sum\n",
    "    threshold_idx = np.where(cum_sum >= percentage)[0][0]\n",
    "    threshold = sorted_attributions[threshold_idx]\n",
    "    if plot_distribution:\n",
    "        raise NotImplementedError \n",
    "    return threshold\n",
    "\n",
    "def polarity_function(attributions, polarity):\n",
    "    if polarity == 'positive':\n",
    "        return np.clip(attributions, 0, 1)\n",
    "    elif polarity == 'negative':\n",
    "        return np.clip(attributions, -1, 0)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def overlay_function(attributions, image):\n",
    "    return np.clip(0.7 * image + 0.5 * attributions, 0, 255)\n",
    "\n",
    "def visualize(attributions, image, positive_channel=G, negative_channel=R, polarity='positive', \\\n",
    "                clip_above_percentile=99.9, clip_below_percentile=0, morphological_cleanup=False, \\\n",
    "                structure=np.ones((3, 3)), outlines=False, outlines_component_percentage=90, overlay=True, \\\n",
    "                mask_mode=False, plot_distribution=False):\n",
    "    if polarity == 'both':\n",
    "        raise NotImplementedError\n",
    "\n",
    "    elif polarity == 'positive':\n",
    "        attributions = polarity_function(attributions, polarity=polarity)\n",
    "        channel = positive_channel\n",
    "    \n",
    "    # convert the attributions to the gray scale\n",
    "    attributions = convert_to_gray_scale(attributions)\n",
    "    attributions = linear_transform(attributions, clip_above_percentile, clip_below_percentile, 0.0, plot_distribution=plot_distribution)\n",
    "    attributions_mask = attributions.copy()\n",
    "    if morphological_cleanup:\n",
    "        raise NotImplementedError\n",
    "    if outlines:\n",
    "        raise NotImplementedError\n",
    "    attributions = np.expand_dims(attributions, 2) * channel\n",
    "    if overlay:\n",
    "        if mask_mode == False:\n",
    "            attributions = overlay_function(attributions, image)\n",
    "        else:\n",
    "            attributions = np.expand_dims(attributions_mask, 2)\n",
    "            attributions = np.clip(attributions * image, 0, 255)\n",
    "            attributions = attributions[:, :, (2, 1, 0)]\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing NMC label [0] and APTOS label 0\n",
      "\n",
      "Processing APTOS label 0 with corresponding NMC labels [0]\n",
      "Saved visualization: visualization_results/comparison/label_0_vs_0/both_correct_0.png\n",
      "Saved visualization: visualization_results/comparison/label_0_vs_0/both_correct_1.png\n",
      "Saved visualization: visualization_results/comparison/label_0_vs_0/both_correct_2.png\n",
      "\n",
      "Processing NMC label [2] and APTOS label 1\n",
      "\n",
      "Processing APTOS label 1 with corresponding NMC labels [2]\n",
      "Found label image: nmc_labeling/e9ff9352ccb3 - 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_2_vs_1/both_correct_0.png\n",
      "Found label image: nmc_labeling/eba3acc42197 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_2_vs_1/both_correct_1.png\n",
      "Found label image: nmc_labeling/ea15a290eb96 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_2_vs_1/only_aptos_correct_0.png\n",
      "Found label image: nmc_labeling/d06ccd0cf4b8 - 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_2_vs_1/both_wrong_0.png\n",
      "\n",
      "Processing NMC label [1] and APTOS label 2\n",
      "\n",
      "Processing APTOS label 2 with corresponding NMC labels [1]\n",
      "Found label image: nmc_labeling/eb1ad14dd281 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_1_vs_2/both_correct_0.png\n",
      "Found label image: nmc_labeling/ee02294cc3d9 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_1_vs_2/both_correct_1.png\n",
      "Found label image: nmc_labeling/d28bd830c171 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_1_vs_2/only_aptos_correct_0.png\n",
      "\n",
      "Processing NMC label [1, 2] and APTOS label 3\n",
      "\n",
      "Processing APTOS label 3 with corresponding NMC labels [1, 2]\n",
      "Found label image: nmc_labeling/f092febbf5c0 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_1-2_vs_3/both_correct_0.png\n",
      "Found label image: nmc_labeling/e13412678eff 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_1-2_vs_3/both_correct_1.png\n",
      "Found label image: nmc_labeling/e0b5a982a018 - 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_1-2_vs_3/only_nmc_correct_0.png\n",
      "\n",
      "Processing NMC label [5, 6] and APTOS label 4\n",
      "\n",
      "Processing APTOS label 4 with corresponding NMC labels [5, 6]\n",
      "Found label image: nmc_labeling/f850cb51fdba 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_5-6_vs_4/only_aptos_correct_0.png\n",
      "Found label image: nmc_labeling/e23add229074 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_5-6_vs_4/only_aptos_correct_1.png\n",
      "Found label image: nmc_labeling/fb696a8e055a 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_5-6_vs_4/both_wrong_0.png\n",
      "Found label image: nmc_labeling/0083ee8054ee - 검은 원(Hard Exudates) 붉은원(Hemorrhages) 초록원 (Microaneurysms)노란원 (Cotton Wool Spots) 하얀원 (laser scar).png\n",
      "Saved visualization: visualization_results/comparison/label_5-6_vs_4/both_wrong_1.png\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "nmc_labels = [[0],[2],[1],[1,2],[5,6]]\n",
    "aptos_labels = [0,1,2,3,4]  # 각각 대응되는 APTOS 라벨\n",
    "\n",
    "for idx, nmc_label_idx in enumerate(nmc_labels):\n",
    "   aptos_label_idx = aptos_labels[idx]\n",
    "   \n",
    "   print(f\"\\nProcessing NMC label {nmc_label_idx} and APTOS label {aptos_label_idx}\")\n",
    "   \n",
    "   # Load NMC model\n",
    "   nmc_model = models.efficientnet_v2_m(pretrained=True)\n",
    "   num_ftrs = nmc_model.classifier[1].in_features\n",
    "   nmc_model.classifier = nn.Sequential(\n",
    "       nn.BatchNorm1d(num_ftrs),\n",
    "       nn.Linear(num_ftrs, len(nmc_label_idx))\n",
    "   )\n",
    "   nmc_model = nmc_model.to(device)\n",
    "   \n",
    "   if len(nmc_label_idx)==1:\n",
    "       nmc_model.load_state_dict(torch.load(f'model/singlelabel/best_model_label_{nmc_label_idx[0]}_nmc_cnn.pth'))\n",
    "   else:\n",
    "       nmc_model.load_state_dict(torch.load(f'model/singlelabel/best_model_labels_{\"-\".join(map(str,nmc_label_idx))}_nmc_cnn.pth'))\n",
    "   \n",
    "   # Load APTOS model\n",
    "   aptos_model = models.efficientnet_v2_m(pretrained=True)\n",
    "   aptos_model.classifier = nn.Sequential(\n",
    "       nn.BatchNorm1d(num_ftrs),\n",
    "       nn.Linear(num_ftrs, 1)\n",
    "   )\n",
    "   aptos_model = aptos_model.to(device)\n",
    "   aptos_model.load_state_dict(torch.load(f'model/singlelabel/best_model_label_{aptos_label_idx}_aptos_cnn.pth'))\n",
    "   \n",
    "   # Compare and save results\n",
    "   #compare_and_save_gradcam(nmc_model, aptos_model, testloader, nmc_label_idx, aptos_label_idx, device,save_dir='grad_label_results')\n",
    "   compare_and_save_visualizations(\n",
    "        nmc_model, \n",
    "        aptos_model, \n",
    "        testloader, \n",
    "        nmc_label_idx, \n",
    "        aptos_label_idx, \n",
    "        device,\n",
    "        save_dir='visualization_results'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
