{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 싱글모델로 분류"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch \n",
        "import argparse\n",
        "import yaml\n",
        "import time\n",
        "import multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "from tabulate import tabulate\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "#from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DistributedSampler, RandomSampler\n",
        "from torch import distributed as dist\n",
        "from nmc.models import *\n",
        "from nmc.datasets import * \n",
        "from nmc.augmentations import get_train_augmentation, get_val_augmentation\n",
        "from nmc.losses import get_loss\n",
        "from nmc.schedulers import get_scheduler\n",
        "from nmc.optimizers import get_optimizer\n",
        "from nmc.utils.utils import fix_seeds, setup_cudnn, cleanup_ddp, setup_ddp\n",
        "from tools.val import evaluate_epi\n",
        "from nmc.utils.episodic_utils import * \n",
        "from scipy.cluster import hierarchy\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mutual_info_score\n",
        "from scipy.cluster import hierarchy\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torch.utils.data import Subset\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import random\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('../configs/NMC.yaml') as f:\n",
        "    cfg = yaml.load(f, Loader=yaml.SafeLoader)\n",
        "print(cfg)\n",
        "fix_seeds(3407)\n",
        "setup_cudnn()\n",
        "gpu = setup_ddp()\n",
        "save_dir = Path(cfg['SAVE_DIR'])\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "cleanup_ddp()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early Stopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_score):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_score\n",
        "        elif val_score < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = val_score\n",
        "            self.counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_train_augmentation(size):\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.Lambda(lambda x: x.float() if x.dtype == torch.uint8 else x),\n",
        "        transforms.Lambda(lambda x: x / 255.0 if x.max() > 1.0 else x),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "def get_val_test_transform(size):\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.Lambda(lambda x: x.float() if x.dtype == torch.uint8 else x),\n",
        "        transforms.Lambda(lambda x: x / 255.0 if x.max() > 1.0 else x),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiTargetBalancedBatchSampler(Sampler):\n",
        "    def __init__(self, dataset, batch_size, target_classes):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.target_classes = target_classes\n",
        "        \n",
        "        # 데이터셋에서 레이블 추출\n",
        "        if hasattr(dataset, 'labels'):\n",
        "            self.labels = dataset.labels\n",
        "            if isinstance(self.labels, np.ndarray):\n",
        "                self.labels = torch.from_numpy(self.labels)\n",
        "        elif hasattr(dataset, 'targets'):\n",
        "            self.labels = dataset.targets\n",
        "            if isinstance(self.labels, np.ndarray):\n",
        "                self.labels = torch.from_numpy(self.labels)\n",
        "        else:\n",
        "            try:\n",
        "                self.labels = [sample[1] for sample in dataset]\n",
        "                if isinstance(self.labels[0], np.ndarray):\n",
        "                    self.labels = torch.from_numpy(np.array(self.labels))\n",
        "                else:\n",
        "                    self.labels = torch.tensor(self.labels)\n",
        "            except:\n",
        "                raise ValueError(\"Cannot access labels from dataset\")\n",
        "        \n",
        "        # 각 타겟 클래스와 나머지 클래스의 인덱스 저장\n",
        "        self.target_indices = {}\n",
        "        for target in target_classes:\n",
        "            if len(self.labels.shape) > 1:\n",
        "                self.target_indices[target] = torch.where(self.labels[:, target] == 1)[0]\n",
        "            else:\n",
        "                self.target_indices[target] = torch.where(self.labels == target)[0]\n",
        "        \n",
        "        # 나머지 클래스의 인덱스 저장\n",
        "        if len(self.labels.shape) > 1:\n",
        "            self.other_indices = torch.where(\n",
        "                torch.sum(self.labels[:, target_classes], dim=1) == 0)[0]\n",
        "        else:\n",
        "            mask = torch.ones_like(self.labels, dtype=torch.bool)\n",
        "            for target in target_classes:\n",
        "                mask &= (self.labels != target)\n",
        "            self.other_indices = torch.where(mask)[0]\n",
        "        \n",
        "        # 각 그룹당 샘플 수 계산\n",
        "        n_groups = len(target_classes) + 1  # 타겟 클래스들 + 나머지\n",
        "        self.samples_per_group = batch_size // n_groups\n",
        "        \n",
        "        self.n_batches = len(self.dataset) // batch_size\n",
        "        if len(self.dataset) % batch_size != 0:\n",
        "            self.n_batches += 1\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for _ in range(self.n_batches):\n",
        "            batch_indices = []\n",
        "            \n",
        "            # 각 타겟 클래스에서 샘플링\n",
        "            for target in self.target_classes:\n",
        "                target_selected = self.target_indices[target][\n",
        "                    torch.randint(len(self.target_indices[target]), \n",
        "                                (self.samples_per_group,))\n",
        "                ]\n",
        "                batch_indices.extend(target_selected.tolist())\n",
        "            \n",
        "            # 나머지 클래스들에서 샘플링\n",
        "            other_selected = self.other_indices[\n",
        "                torch.randint(len(self.other_indices), \n",
        "                            (self.samples_per_group,))\n",
        "            ]\n",
        "            batch_indices.extend(other_selected.tolist())\n",
        "            \n",
        "            # 배치 셔플\n",
        "            random.shuffle(batch_indices)\n",
        "            \n",
        "            # 배치 크기에 맞게 자르기 (나누어 떨어지지 않는 경우 처리)\n",
        "            if len(batch_indices) > self.batch_size:\n",
        "                batch_indices = batch_indices[:self.batch_size]\n",
        "            \n",
        "            yield batch_indices\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, scaler, device, target_label_idx):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    num_targets = len(target_label_idx)\n",
        "\n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        images = images.to(device)\n",
        "        \n",
        "        if num_targets == 1:\n",
        "            # 단일 레이블 케이스\n",
        "            target_labels = labels[:, target_label_idx].to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            with autocast(enabled=True):\n",
        "                outputs = model(images)\n",
        "                # 차원을 맞춰줌\n",
        "                outputs = outputs.view(-1)  # or outputs.squeeze()\n",
        "                target_labels = target_labels.view(-1)  # or target_labels.squeeze()\n",
        "                loss = criterion(outputs, target_labels)\n",
        "        else:\n",
        "            # 다중 레이블 케이스\n",
        "            target_labels = labels[:, target_label_idx].to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            with autocast(enabled=True):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, target_labels)\n",
        "\n",
        "        if scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, device, target_label_idx):\n",
        "   model.eval()\n",
        "   all_preds = []\n",
        "   all_labels = []\n",
        "   num_targets = len(target_label_idx)\n",
        "   \n",
        "   with torch.no_grad():\n",
        "       for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "           images = images.to(device)\n",
        "           \n",
        "           if num_targets == 1:\n",
        "               # 단일 레이블 케이스\n",
        "               target_labels = labels[:, target_label_idx].to(device)\n",
        "               outputs = model(images)\n",
        "               \n",
        "               # 차원 처리\n",
        "               if len(outputs.shape) == 2:\n",
        "                   outputs = outputs.squeeze(1)\n",
        "               \n",
        "               preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "               \n",
        "               all_preds.append(preds.cpu().numpy().reshape(-1))\n",
        "               all_labels.append(target_labels.cpu().numpy().reshape(-1))\n",
        "           \n",
        "           else:\n",
        "               # 다중 레이블 케이스\n",
        "               target_labels = labels[:, target_label_idx].to(device)\n",
        "               outputs = model(images)\n",
        "               \n",
        "               # 각 레이블에 대한 예측\n",
        "               preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "               \n",
        "               all_preds.append(preds.cpu().numpy())\n",
        "               all_labels.append(target_labels.cpu().numpy())\n",
        "   \n",
        "   # numpy array로 변환\n",
        "   all_preds = np.concatenate(all_preds)\n",
        "   all_labels = np.concatenate(all_labels)\n",
        "   \n",
        "   if num_targets == 1:\n",
        "       # 단일 레이블 메트릭\n",
        "       f1 = f1_score(all_labels, all_preds, average='binary')\n",
        "       accuracy = accuracy_score(all_labels, all_preds)\n",
        "       precision = precision_score(all_labels, all_preds)\n",
        "       recall = recall_score(all_labels, all_preds)\n",
        "       \n",
        "       return f1, accuracy, precision, recall\n",
        "   else:\n",
        "       # 다중 레이블 메트릭\n",
        "       f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "       accuracy = accuracy_score(all_labels, all_preds)\n",
        "       precision = precision_score(all_labels, all_preds, average='macro')\n",
        "       recall = recall_score(all_labels, all_preds, average='macro')\n",
        "       \n",
        "       return f1, accuracy, precision, recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, scaler, device, epochs, target_label_idx):\n",
        "    best_f1 = 0.0\n",
        "    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
        "    num_targets = len(target_label_idx)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        \n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, scaler, device, target_label_idx)\n",
        "        \n",
        "        if num_targets == 1:\n",
        "            val_f1, val_acc, val_prec, val_rec = evaluate(model, val_loader, device, target_label_idx)\n",
        "            \n",
        "            print(f\"Training Loss: {train_loss:.4f}\")\n",
        "            print(f\"Validation Metrics:\")\n",
        "            print(f\"  F1 Score: {val_f1:.4f}\")\n",
        "            print(f\"  Accuracy: {val_acc:.4f}\") \n",
        "            print(f\"  Precision: {val_prec:.4f}\")\n",
        "            print(f\"  Recall: {val_rec:.4f}\")\n",
        "            \n",
        "            scheduler.step(val_f1)\n",
        "            \n",
        "            if val_f1 > best_f1:\n",
        "                best_f1 = val_f1\n",
        "                torch.save(model.state_dict(), f'model/singlelabel/best_model_label_{target_label_idx[0]}_nmc_cnn.pth')\n",
        "                print(\"New best model saved!\")\n",
        "        else:\n",
        "            # 모든 메트릭을 받아서 f1만 사용\n",
        "            val_f1, val_acc, val_prec, val_rec = evaluate(model, val_loader, device, target_label_idx)\n",
        "            \n",
        "            print(f\"Training Loss: {train_loss:.4f}\")\n",
        "            print(f\"Validation Metrics:\")\n",
        "            print(f\"  Macro F1 Score: {val_f1:.4f}\")\n",
        "            print(f\"  Macro Accuracy: {val_acc:.4f}\")\n",
        "            print(f\"  Macro Precision: {val_prec:.4f}\")\n",
        "            print(f\"  Macro Recall: {val_rec:.4f}\")\n",
        "            \n",
        "            scheduler.step(val_f1)\n",
        "            \n",
        "            if val_f1 > best_f1:\n",
        "                best_f1 = val_f1\n",
        "                torch.save(model.state_dict(), f'model/singlelabel/best_model_labels_{\"-\".join(map(str,target_label_idx))}_nmc_cnn.pth')\n",
        "                print(\"New best model saved!\")\n",
        "        \n",
        "        early_stopping(val_f1)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "            \n",
        "        print()\n",
        "    \n",
        "    return best_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "best_mf1 = 0.0\n",
        "device = torch.device(cfg['DEVICE'])\n",
        "device = \"cuda:1\"\n",
        "print(\"device : \", device)\n",
        "num_workers = mp.cpu_count()\n",
        "train_cfg, eval_cfg = cfg['TRAIN'], cfg['EVAL']\n",
        "dataset_cfg, model_cfg = cfg['DATASET'], cfg['MODEL']\n",
        "loss_cfg, optim_cfg, sched_cfg = cfg['LOSS'], cfg['OPTIMIZER'], cfg['SCHEDULER']\n",
        "epochs, lr = train_cfg['EPOCHS'], optim_cfg['LR']\n",
        "\n",
        "image_size = [256,256]\n",
        "image_dir = Path(dataset_cfg['ROOT']) / 'train_images'\n",
        "train_transform = get_train_augmentation(image_size)\n",
        "val_test_transform = get_val_test_transform(image_size)\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "dataset = eval(dataset_cfg['NAME'])(\n",
        "    dataset_cfg['ROOT'] + '/cropped_images_1424x1648',\n",
        "    dataset_cfg['TRAIN_RATIO'],\n",
        "    dataset_cfg['VALID_RATIO'],\n",
        "    dataset_cfg['TEST_RATIO'],\n",
        "    transform=None\n",
        ")\n",
        "trainset, valset, testset = dataset.get_splits()\n",
        "valset.transform = val_test_transform\n",
        "testset.transform = val_test_transform\n",
        "\n",
        "# trainloader = DataLoader(trainset, batch_size=batch_size, num_workers=num_workers, drop_last=True, pin_memory=True)\n",
        "valloader = DataLoader(valset, batch_size=1, num_workers=1, pin_memory=True)\n",
        "testloader = DataLoader(testset, batch_size=1, num_workers=1, pin_memory=True)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import os\n",
        "def denormalize(tensor):\n",
        "   \"\"\"Denormalize the image tensor\"\"\"\n",
        "   mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "   std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "   return tensor * std + mean\n",
        "\n",
        "def occlusion_sensitivity_map(model, image, target_label_idx, patch_size=14, stride=7, device='cuda'):\n",
        "    model.eval()\n",
        "    height, width = image.shape[-2:]\n",
        "    sensitivity_map = torch.zeros((height, width), device=device)\n",
        "    \n",
        "    # 타겟 인덱스 처리 수정\n",
        "    if isinstance(target_label_idx, list):\n",
        "        target_idx = target_label_idx[0]\n",
        "    else:\n",
        "        target_idx = target_label_idx\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        base_output = model(image.unsqueeze(0))\n",
        "        base_pred = torch.sigmoid(base_output)[0, target_idx].item()\n",
        "   \n",
        "    # 패치 단위로 가리며 예측값 변화 측정\n",
        "    for y in range(0, height-patch_size+1, stride):\n",
        "        for x in range(0, width-patch_size+1, stride):\n",
        "            occluded = image.clone()\n",
        "            occluded[..., y:y+patch_size, x:x+patch_size] = 0\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                output = model(occluded.unsqueeze(0))\n",
        "                pred = torch.sigmoid(output)[0, target_label_idx].item()\n",
        "            \n",
        "            diff = abs(base_pred - pred)\n",
        "            sensitivity_map[y:y+patch_size, x:x+patch_size] += diff\n",
        "            \n",
        "    # 정규화\n",
        "    sensitivity_map = (sensitivity_map - sensitivity_map.min()) / (sensitivity_map.max() - sensitivity_map.min() + 1e-8)\n",
        "    return sensitivity_map.cpu().numpy()\n",
        "\n",
        "def create_comparison_image(original_img, nmc_heatmap, aptos_heatmap, label_info, nmc_correct, aptos_correct, save_path):\n",
        "   # Denormalize the original image\n",
        "   orig_img = denormalize(torch.from_numpy(original_img)).numpy()\n",
        "   orig_img = np.clip(orig_img.transpose(1, 2, 0), 0, 1)\n",
        "   orig_img = np.uint8(orig_img * 255)\n",
        "   \n",
        "   # Create heatmap overlays\n",
        "   nmc_heatmap_rgb = np.uint8(255 * nmc_heatmap)\n",
        "   nmc_heatmap_rgb = cv2.applyColorMap(nmc_heatmap_rgb, cv2.COLORMAP_JET)\n",
        "   nmc_superimposed = cv2.addWeighted(orig_img, 0.6, nmc_heatmap_rgb, 0.4, 0)\n",
        "   \n",
        "   aptos_heatmap_rgb = np.uint8(255 * aptos_heatmap)\n",
        "   aptos_heatmap_rgb = cv2.applyColorMap(aptos_heatmap_rgb, cv2.COLORMAP_JET)\n",
        "   aptos_superimposed = cv2.addWeighted(orig_img, 0.6, aptos_heatmap_rgb, 0.4, 0)\n",
        "   \n",
        "   # Create a white background for label info\n",
        "   height, width = orig_img.shape[:2]\n",
        "   info_height = height // 2  # 라벨 정보 영역의 높이를 절반으로\n",
        "   info_bg = np.ones((info_height, width * 3, 3), dtype=np.uint8) * 255  # 3배 너비\n",
        "   \n",
        "   # Add text to info background\n",
        "   font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "   font_scale = 0.5  # 글씨 크기 축소\n",
        "   thickness = 1\n",
        "   color = (0, 0, 0)  # Black color\n",
        "   \n",
        "   # Add label information\n",
        "   y_offset = 20\n",
        "   for line in label_info:\n",
        "       text_size = cv2.getTextSize(line, font, font_scale, thickness)[0]\n",
        "       x = (info_bg.shape[1] - text_size[0]) // 2\n",
        "       y = y_offset\n",
        "       cv2.putText(info_bg, line, (x, y), font, font_scale, color, thickness)\n",
        "       y_offset += 20\n",
        "   \n",
        "   # Create result texts\n",
        "   nmc_result = \"NMC: Correct\" if nmc_correct else \"NMC: Incorrect\"\n",
        "   aptos_result = \"APTOS: Correct\" if aptos_correct else \"APTOS: Incorrect\"\n",
        "   \n",
        "   # Add model results with color coding\n",
        "   text_size = cv2.getTextSize(nmc_result, font, font_scale, thickness)[0]\n",
        "   x = width + (width - text_size[0]) // 2\n",
        "   y = height // 2 - 20\n",
        "   color = (0, 255, 0) if nmc_correct else (0, 0, 255)\n",
        "   cv2.putText(info_bg, nmc_result, (x, y), font, font_scale, color, thickness)\n",
        "   \n",
        "   text_size = cv2.getTextSize(aptos_result, font, font_scale, thickness)[0]\n",
        "   x = 2 * width + (width - text_size[0]) // 2\n",
        "   color = (0, 255, 0) if aptos_correct else (0, 0, 255)\n",
        "   cv2.putText(info_bg, aptos_result, (x, y), font, font_scale, color, thickness)\n",
        "   \n",
        "   # Concatenate images horizontally\n",
        "   images_row = np.concatenate([orig_img, nmc_superimposed, aptos_superimposed], axis=1)\n",
        "   \n",
        "   # Concatenate info and images vertically\n",
        "   final_image = np.concatenate([info_bg, images_row], axis=0)\n",
        "   \n",
        "   # Save the final image\n",
        "   cv2.imwrite(save_path, cv2.cvtColor(final_image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "def compare_and_save_gradcam(nmc_model, aptos_model, dataloader, nmc_label_idx, aptos_label_idx, device, save_dir='osm_results'):\n",
        "   nmc_model.eval()\n",
        "   aptos_model.eval()\n",
        "   \n",
        "   both_correct = []\n",
        "   only_nmc_correct = []\n",
        "   only_aptos_correct = []\n",
        "   both_wrong = []\n",
        "   \n",
        "   save_path = f\"{save_dir}/comparison/label_{'-'.join(map(str, nmc_label_idx))}_vs_{aptos_label_idx}\"\n",
        "   os.makedirs(save_path, exist_ok=True)\n",
        "   \n",
        "   with torch.no_grad():\n",
        "       for batch_idx, (images, labels) in enumerate(dataloader):\n",
        "           images = images.to(device)\n",
        "           labels = labels.to(device)\n",
        "           \n",
        "           # APTOS model predictions (기준)\n",
        "           aptos_outputs = aptos_model(images)\n",
        "           aptos_predictions = (torch.sigmoid(aptos_outputs) > 0.5).squeeze()\n",
        "           aptos_raw_preds = torch.sigmoid(aptos_outputs).squeeze()\n",
        "           aptos_targets = labels[:, aptos_label_idx]\n",
        "           \n",
        "           # NMC model predictions\n",
        "           nmc_outputs = nmc_model(images)\n",
        "           nmc_predictions = (torch.sigmoid(nmc_outputs) > 0.5)\n",
        "           nmc_raw_preds = torch.sigmoid(nmc_outputs)\n",
        "           \n",
        "           # APTOS 라벨을 기준으로 NMC 예측 평가\n",
        "           nmc_correct = torch.where(\n",
        "               aptos_targets == 1,\n",
        "               torch.all(nmc_predictions == 1, dim=1),  # APTOS가 1일 때\n",
        "               torch.all(nmc_predictions == 0, dim=1)   # APTOS가 0일 때\n",
        "           )\n",
        "           \n",
        "           # Handle batch size 1 case\n",
        "           if len(images) == 1:\n",
        "               aptos_predictions = aptos_predictions.unsqueeze(0)\n",
        "               aptos_raw_preds = aptos_raw_preds.unsqueeze(0)\n",
        "               aptos_targets = aptos_targets.unsqueeze(0)\n",
        "               nmc_correct = nmc_correct.unsqueeze(0)\n",
        "           \n",
        "           for i in range(len(images)):\n",
        "               try:\n",
        "                   aptos_correct = (aptos_predictions[i].item() == aptos_targets[i].item())\n",
        "                   nmc_is_correct = nmc_correct[i].item()\n",
        "                   \n",
        "                   sample_info = {\n",
        "                       'image': images[i],\n",
        "                       'aptos_target': aptos_targets[i].item(),\n",
        "                       'nmc_preds': nmc_raw_preds[i].cpu().numpy(),\n",
        "                       'aptos_pred': aptos_raw_preds[i].item()\n",
        "                   }\n",
        "                   \n",
        "                   # Categorize samples\n",
        "                   if aptos_correct and nmc_is_correct and len(both_correct) < 3:\n",
        "                       both_correct.append(sample_info)\n",
        "                   elif not aptos_correct and nmc_is_correct and len(only_nmc_correct) < 3:\n",
        "                       only_nmc_correct.append(sample_info)\n",
        "                   elif aptos_correct and not nmc_is_correct and len(only_aptos_correct) < 3:\n",
        "                       only_aptos_correct.append(sample_info)\n",
        "                   elif not aptos_correct and not nmc_is_correct and len(both_wrong) < 3:\n",
        "                       both_wrong.append(sample_info)\n",
        "               \n",
        "               except Exception as e:\n",
        "                   print(f\"Error processing sample {i} in batch {batch_idx}: {e}\")\n",
        "                   continue\n",
        "               \n",
        "               if (len(both_correct) >= 3 and \n",
        "                   len(only_nmc_correct) >= 3 and \n",
        "                   len(only_aptos_correct) >= 3 and \n",
        "                   len(both_wrong) >= 3):\n",
        "                   break\n",
        "                   \n",
        "           if (len(both_correct) >= 3 and \n",
        "               len(only_nmc_correct) >= 3 and \n",
        "               len(only_aptos_correct) >= 3 and \n",
        "               len(both_wrong) >= 3):\n",
        "               break\n",
        "   \n",
        "   print(f\"\\nSamples collected:\")\n",
        "   print(f\"Both correct: {len(both_correct)}\")\n",
        "   print(f\"Only NMC correct: {len(only_nmc_correct)}\")\n",
        "   print(f\"Only APTOS correct: {len(only_aptos_correct)}\")\n",
        "   print(f\"Both wrong: {len(both_wrong)}\")\n",
        "   \n",
        "   # Save GradCAM visualizations for all categories\n",
        "   for category, samples, category_name in [\n",
        "       (both_correct, \"both_correct\", \"Both Correct\"),\n",
        "       (only_nmc_correct, \"only_nmc\", \"Only NMC Correct\"),\n",
        "       (only_aptos_correct, \"only_aptos\", \"Only APTOS Correct\"),\n",
        "       (both_wrong, \"both_wrong\", \"Both Wrong\")\n",
        "   ]:\n",
        "       for idx, sample in enumerate(category):\n",
        "           try:\n",
        "               nmc_heatmap = occlusion_sensitivity_map(nmc_model, sample['image'], nmc_label_idx[0], device=device)\n",
        "               aptos_heatmap = occlusion_sensitivity_map(aptos_model, sample['image'], 0, device=device)\n",
        "\n",
        "               \n",
        "               # Prepare label info\n",
        "               nmc_pred_str = np.array2string(sample['nmc_preds'], precision=3)\n",
        "               aptos_pred_str = f\"{sample['aptos_pred']:.3f}\"\n",
        "               aptos_target_str = f\"{int(sample['aptos_target'])}\"\n",
        "               \n",
        "               nmc_is_correct = torch.all(torch.tensor(sample['nmc_preds'] > 0.5) == (sample['aptos_target'] == 1))\n",
        "               aptos_is_correct = (sample['aptos_pred'] > 0.5) == sample['aptos_target']\n",
        "               \n",
        "               label_info = [\n",
        "                   f\"Category: {category_name}\",\n",
        "                   f\"NMC Labels: {nmc_label_idx}, APTOS Label: {aptos_label_idx}\",\n",
        "                   f\"APTOS - True: {aptos_target_str}, Pred: {aptos_pred_str}\",\n",
        "                   f\"NMC Predictions: {nmc_pred_str}\"\n",
        "               ]\n",
        "               \n",
        "               # Save combined result\n",
        "               save_name = os.path.join(save_path, f'{category_name.lower().replace(\" \", \"_\")}_{idx}.png')\n",
        "               create_comparison_image(\n",
        "                   sample['image'].cpu().numpy(),\n",
        "                   nmc_heatmap,\n",
        "                   aptos_heatmap,\n",
        "                   label_info,\n",
        "                   nmc_is_correct,\n",
        "                   aptos_is_correct,\n",
        "                   save_name\n",
        "               )\n",
        "               \n",
        "           except Exception as e:\n",
        "               print(f\"Error processing {category_name} sample {idx}: {e}\")\n",
        "               print(f\"Sample info: {sample}\")\n",
        "               continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main execution\n",
        "nmc_labels = [[0],[2],[1],[1,2],[5,6]]\n",
        "aptos_labels = [0,1,2,3,4]  # 각각 대응되는 APTOS 라벨\n",
        "\n",
        "for idx, nmc_label_idx in enumerate(nmc_labels):\n",
        "   aptos_label_idx = aptos_labels[idx]\n",
        "   \n",
        "   print(f\"\\nProcessing NMC label {nmc_label_idx} and APTOS label {aptos_label_idx}\")\n",
        "   \n",
        "   # Load NMC model\n",
        "   nmc_model = models.efficientnet_v2_m(pretrained=True)\n",
        "   num_ftrs = nmc_model.classifier[1].in_features\n",
        "   nmc_model.classifier = nn.Sequential(\n",
        "       nn.BatchNorm1d(num_ftrs),\n",
        "       nn.Linear(num_ftrs, len(nmc_label_idx))\n",
        "   )\n",
        "   nmc_model = nmc_model.to(device)\n",
        "   \n",
        "   if len(nmc_label_idx)==1:\n",
        "       nmc_model.load_state_dict(torch.load(f'model/singlelabel/best_model_label_{nmc_label_idx[0]}_nmc_cnn.pth'))\n",
        "   else:\n",
        "       nmc_model.load_state_dict(torch.load(f'model/singlelabel/best_model_labels_{\"-\".join(map(str,nmc_label_idx))}_nmc_cnn.pth'))\n",
        "   \n",
        "   # Load APTOS model\n",
        "   aptos_model = models.efficientnet_v2_m(pretrained=True)\n",
        "   aptos_model.classifier = nn.Sequential(\n",
        "       nn.BatchNorm1d(num_ftrs),\n",
        "       nn.Linear(num_ftrs, 1)\n",
        "   )\n",
        "   aptos_model = aptos_model.to(device)\n",
        "   aptos_model.load_state_dict(torch.load(f'model/singlelabel/best_model_label_{aptos_label_idx}_aptos_cnn.pth'))\n",
        "   \n",
        "   # Compare and save results\n",
        "   compare_and_save_gradcam(nmc_model, aptos_model, testloader, nmc_label_idx, aptos_label_idx, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}