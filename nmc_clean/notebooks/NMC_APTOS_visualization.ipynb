{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 싱글모델로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import argparse\n",
    "import yaml\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "from torch import distributed as dist\n",
    "from nmc.models import *\n",
    "from nmc.datasets import * \n",
    "from nmc.augmentations import get_train_augmentation, get_val_augmentation\n",
    "from nmc.losses import get_loss\n",
    "from nmc.schedulers import get_scheduler\n",
    "from nmc.optimizers import get_optimizer\n",
    "from nmc.utils.utils import fix_seeds, setup_cudnn, cleanup_ddp, setup_ddp\n",
    "from tools.val import evaluate_epi\n",
    "from nmc.utils.episodic_utils import * \n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.cluster import hierarchy\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DEVICE': 'cuda:0', 'SAVE_DIR': 'output', 'MODEL': {'NAME': 'EfficientNetV2MModel', 'BACKBONE': 'EfficientNetV2', 'PRETRAINED': '/workspace/jhmoon/nmc_2024/checkpoints/pretrained/tf_efficientnetv2_m_weights.pth', 'UNFREEZE': 'full', 'VERSION': '384_32'}, 'DATASET': {'NAME': 'APTOSDataset', 'ROOT': '/data/public_data/aptos', 'TRAIN_RATIO': 0.7, 'VALID_RATIO': 0.15, 'TEST_RATIO': 0.15}, 'TRAIN': {'IMAGE_SIZE': [384, 384], 'BATCH_SIZE': 32, 'EPOCHS': 100, 'EVAL_INTERVAL': 25, 'AMP': False, 'DDP': False}, 'LOSS': {'NAME': 'CrossEntropy', 'CLS_WEIGHTS': False}, 'OPTIMIZER': {'NAME': 'adamw', 'LR': 0.001, 'WEIGHT_DECAY': 0.01}, 'SCHEDULER': {'NAME': 'warmuppolylr', 'POWER': 0.9, 'WARMUP': 10, 'WARMUP_RATIO': 0.1}, 'EVAL': {'MODEL_PATH': 'checkpoints/pretrained/FGMaxxVit/FGMaxxVit.FGMaxxVit.APTOS.pth', 'IMAGE_SIZE': [384, 384]}, 'TEST': {'MODEL_PATH': 'checkpoints/pretrained/FGMaxxVit/FGMaxxVit.FGMaxxVit.APTOS.pth', 'FILE': 'assests/ade', 'IMAGE_SIZE': [384, 384], 'OVERLAY': True}}\n"
     ]
    }
   ],
   "source": [
    "with open('../configs/APTOS.yaml') as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "print(cfg)\n",
    "fix_seeds(3407)\n",
    "setup_cudnn()\n",
    "gpu = setup_ddp()\n",
    "save_dir = Path(cfg['SAVE_DIR'])\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "cleanup_ddp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_augmentation(size):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.Lambda(lambda x: x.float() if x.dtype == torch.uint8 else x),\n",
    "        transforms.Lambda(lambda x: x / 255.0 if x.max() > 1.0 else x),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_val_test_transform(size):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.Lambda(lambda x: x.float() if x.dtype == torch.uint8 else x),\n",
    "        transforms.Lambda(lambda x: x / 255.0 if x.max() > 1.0 else x),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTargetBalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, target_classes):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.target_classes = target_classes\n",
    "        \n",
    "        # 데이터셋에서 레이블 추출\n",
    "        if hasattr(dataset, 'labels'):\n",
    "            self.labels = dataset.labels\n",
    "            if isinstance(self.labels, np.ndarray):\n",
    "                self.labels = torch.from_numpy(self.labels)\n",
    "        elif hasattr(dataset, 'targets'):\n",
    "            self.labels = dataset.targets\n",
    "            if isinstance(self.labels, np.ndarray):\n",
    "                self.labels = torch.from_numpy(self.labels)\n",
    "        else:\n",
    "            try:\n",
    "                self.labels = [sample[1] for sample in dataset]\n",
    "                if isinstance(self.labels[0], np.ndarray):\n",
    "                    self.labels = torch.from_numpy(np.array(self.labels))\n",
    "                else:\n",
    "                    self.labels = torch.tensor(self.labels)\n",
    "            except:\n",
    "                raise ValueError(\"Cannot access labels from dataset\")\n",
    "        \n",
    "        # 각 타겟 클래스와 나머지 클래스의 인덱스 저장\n",
    "        self.target_indices = {}\n",
    "        for target in target_classes:\n",
    "            if len(self.labels.shape) > 1:\n",
    "                self.target_indices[target] = torch.where(self.labels[:, target] == 1)[0]\n",
    "            else:\n",
    "                self.target_indices[target] = torch.where(self.labels == target)[0]\n",
    "        \n",
    "        # 나머지 클래스의 인덱스 저장\n",
    "        if len(self.labels.shape) > 1:\n",
    "            self.other_indices = torch.where(\n",
    "                torch.sum(self.labels[:, target_classes], dim=1) == 0)[0]\n",
    "        else:\n",
    "            mask = torch.ones_like(self.labels, dtype=torch.bool)\n",
    "            for target in target_classes:\n",
    "                mask &= (self.labels != target)\n",
    "            self.other_indices = torch.where(mask)[0]\n",
    "        \n",
    "        # 각 그룹당 샘플 수 계산\n",
    "        n_groups = len(target_classes) + 1  # 타겟 클래스들 + 나머지\n",
    "        self.samples_per_group = batch_size // n_groups\n",
    "        \n",
    "        self.n_batches = len(self.dataset) // batch_size\n",
    "        if len(self.dataset) % batch_size != 0:\n",
    "            self.n_batches += 1\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for _ in range(self.n_batches):\n",
    "            batch_indices = []\n",
    "            \n",
    "            # 각 타겟 클래스에서 샘플링\n",
    "            for target in self.target_classes:\n",
    "                target_selected = self.target_indices[target][\n",
    "                    torch.randint(len(self.target_indices[target]), \n",
    "                                (self.samples_per_group,))\n",
    "                ]\n",
    "                batch_indices.extend(target_selected.tolist())\n",
    "            \n",
    "            # 나머지 클래스들에서 샘플링\n",
    "            other_selected = self.other_indices[\n",
    "                torch.randint(len(self.other_indices), \n",
    "                            (self.samples_per_group,))\n",
    "            ]\n",
    "            batch_indices.extend(other_selected.tolist())\n",
    "            \n",
    "            # 배치 셔플\n",
    "            random.shuffle(batch_indices)\n",
    "            \n",
    "            # 배치 크기에 맞게 자르기 (나누어 떨어지지 않는 경우 처리)\n",
    "            if len(batch_indices) > self.batch_size:\n",
    "                batch_indices = batch_indices[:self.batch_size]\n",
    "            \n",
    "            yield batch_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda:1\n",
      "/data/public_data/aptos/combined_images\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "best_mf1 = 0.0\n",
    "device = torch.device(cfg['DEVICE'])\n",
    "device = \"cuda:1\"\n",
    "print(\"device : \", device)\n",
    "num_workers = mp.cpu_count()\n",
    "train_cfg, eval_cfg = cfg['TRAIN'], cfg['EVAL']\n",
    "dataset_cfg, model_cfg = cfg['DATASET'], cfg['MODEL']\n",
    "loss_cfg, optim_cfg, sched_cfg = cfg['LOSS'], cfg['OPTIMIZER'], cfg['SCHEDULER']\n",
    "epochs, lr = train_cfg['EPOCHS'], optim_cfg['LR']\n",
    "\n",
    "image_size = [256,256]\n",
    "image_dir = Path(dataset_cfg['ROOT']) / 'train_images'\n",
    "train_transform = get_train_augmentation(image_size)\n",
    "val_test_transform = get_val_test_transform(image_size)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "dataset = eval(dataset_cfg['NAME']+'Test')(\n",
    "    dataset_cfg['ROOT'] + '/combined_images',\n",
    "    transform=None,\n",
    "    target_label=None,\n",
    ")\n",
    "dataset.transform = val_test_transform\n",
    "# trainset, valset, testset = dataset.get_splits()\n",
    "# valset.transform = val_test_transform\n",
    "# testset.transform = val_test_transform\n",
    "\n",
    "# trainloader = DataLoader(trainset, batch_size=batch_size, num_workers=num_workers, drop_last=True, pin_memory=True)\n",
    "# valloader = DataLoader(valset, batch_size=1, num_workers=1, pin_memory=True)\n",
    "testloader = DataLoader(dataset, batch_size=1, num_workers=1, pin_memory=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import os\n",
    "from captum.attr import IntegratedGradients, LayerLRP\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "def denormalize(tensor):\n",
    "   \"\"\"Denormalize the image tensor\"\"\"\n",
    "   mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "   std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "   return tensor * std + mean\n",
    "\n",
    "def get_gradcam(model, image, target_label_idx, device):\n",
    "   target_layer = model.features[-1]\n",
    "   cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "   \n",
    "   # target_label_idx가 리스트인 경우 첫 번째 값만 사용\n",
    "   target = ClassifierOutputTarget(0)  # 단일 클래스만 타겟팅\n",
    "   \n",
    "   grayscale_cam = cam(input_tensor=image.unsqueeze(0),\n",
    "                      targets=[target])\n",
    "   \n",
    "   return grayscale_cam[0]\n",
    "\n",
    "def get_integrated_gradients(model, image, target_label_idx, device):\n",
    "    ig = IntegratedGradients(model)\n",
    "    \n",
    "    image = image.clone().detach().requires_grad_(True).to(device)\n",
    "    image = image.unsqueeze(0)\n",
    "    baseline = torch.zeros_like(image).to(device)\n",
    "    \n",
    "    try:\n",
    "        attributions = ig.attribute(\n",
    "            image,\n",
    "            baseline,\n",
    "            target=0,\n",
    "            n_steps=50\n",
    "        )\n",
    "        \n",
    "        attribution_map = attributions.squeeze().permute(1, 2, 0).cpu().detach().numpy()\n",
    "        denorm_image = denormalize(image.squeeze().cpu().detach()).permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # 원본 이미지를 0-255 범위의 uint8로 변환 후 흑백으로 변환\n",
    "        orig_uint8 = np.uint8(denorm_image * 255)\n",
    "        gray_image = cv2.cvtColor(orig_uint8, cv2.COLOR_RGB2GRAY)\n",
    "        gray_image = cv2.cvtColor(gray_image, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        visualization = visualize(\n",
    "            attributions=attribution_map,\n",
    "            image=gray_image,  # uint8 형식의 흑백 이미지 사용\n",
    "            positive_channel=[255, 0, 0],\n",
    "            negative_channel=[0, 0, 0],\n",
    "            polarity='positive',\n",
    "            clip_above_percentile=99,\n",
    "            clip_below_percentile=70,\n",
    "            overlay=True,\n",
    "            mask_mode=False  # overlay 모드 사용\n",
    "        )\n",
    "        \n",
    "        # 최종 결과를 0-1 범위로 정규화\n",
    "        visualization = visualization / 255.0\n",
    "        \n",
    "        return visualization\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Integrated Gradients: {e}\")\n",
    "        return np.zeros((image.shape[2], image.shape[3], 3))\n",
    "\n",
    "def get_lrp(model, image, target_label_idx, device):\n",
    "    # 모델의 마지막 feature layer를 타겟으로 설정\n",
    "    target_layer = model.features[-1]\n",
    "    lrp = LayerLRP(model, target_layer)\n",
    "    \n",
    "    # requires_grad 설정 및 차원 추가\n",
    "    image = image.clone().detach().requires_grad_(True)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    try:\n",
    "        attributions = lrp.attribute(\n",
    "            image,  # 이미 batch dimension이 추가된 상태\n",
    "            target=0\n",
    "        )\n",
    "        \n",
    "        # 속성값을 시각화 가능한 형태로 변환\n",
    "        attribution_map = torch.sum(torch.abs(attributions), dim=1).squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        # 정규화\n",
    "        if attribution_map.max() != attribution_map.min():\n",
    "            attribution_map = (attribution_map - attribution_map.min()) / (attribution_map.max() - attribution_map.min())\n",
    "        else:\n",
    "            attribution_map = np.zeros_like(attribution_map)\n",
    "        \n",
    "        return attribution_map\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LRP: {e}\")\n",
    "        # 에러 발생 시 zero map 반환\n",
    "        return np.zeros((image.shape[2], image.shape[3]))\n",
    "    \n",
    "def create_visualization_comparison(original_img, methods_results, label_info, is_correct, save_path):\n",
    "    # Denormalize original image\n",
    "    orig_img = denormalize(torch.from_numpy(original_img)).numpy()\n",
    "    orig_img = np.clip(orig_img.transpose(1, 2, 0), 0, 1)\n",
    "    orig_img = np.uint8(orig_img * 255)\n",
    "\n",
    "    # Calculate dimensions\n",
    "    height, width = orig_img.shape[:2]\n",
    "    info_height = height // 8\n",
    "\n",
    "    # Get original image name and find corresponding label image\n",
    "    img_name = [line for line in label_info if \"Image: \" in line][0].split(\"Image: \")[1]\n",
    "    img_base_name = img_name.split('.')[0]\n",
    "    label_img_path = find_label_image(img_base_name)\n",
    "\n",
    "    # Load and resize label image\n",
    "    try:\n",
    "        if label_img_path:\n",
    "            label_img = cv2.imread(label_img_path)\n",
    "            if label_img is not None:\n",
    "                label_img = cv2.cvtColor(label_img, cv2.COLOR_BGR2RGB)\n",
    "                label_img = cv2.resize(label_img, (width, height))\n",
    "            else:\n",
    "                label_img = orig_img.copy()\n",
    "        else:\n",
    "            label_img = orig_img.copy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading label image: {e}\")\n",
    "        label_img = orig_img.copy()\n",
    "\n",
    "    # Create info background\n",
    "    info_bg = np.ones((info_height, width * 4, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    # Add text in single line\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.7\n",
    "    thickness = 1\n",
    "    \n",
    "    combined_text = label_info[0]\n",
    "    text_size = cv2.getTextSize(combined_text, font, font_scale, thickness)[0]\n",
    "    x = (info_bg.shape[1] - text_size[0]) // 2\n",
    "    y = info_height // 2 + 5\n",
    "    cv2.putText(info_bg, combined_text, (x, y), font, font_scale, (0, 0, 0), thickness)\n",
    "\n",
    "    # Create image labels for both rows\n",
    "    top_labels = ['Original', 'GradCAM (NMC)', 'GradCAM (APTOS)', 'GradCAM (FT-APTOS)']\n",
    "    bottom_labels = ['Original with Label', 'IG (NMC)', 'IG (APTOS)', 'IG (FT-APTOS)']\n",
    "\n",
    "    # Create label backgrounds\n",
    "    label_height = 30\n",
    "    top_label_bg = np.ones((label_height, width * 4, 3), dtype=np.uint8) * 255\n",
    "    bottom_label_bg = np.ones((label_height, width * 4, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    # Add labels\n",
    "    for idx, (top_label, bottom_label) in enumerate(zip(top_labels, bottom_labels)):\n",
    "        x = width * idx + (width - cv2.getTextSize(top_label, font, font_scale, thickness)[0][0]) // 2\n",
    "        cv2.putText(top_label_bg, top_label, (x, 20), font, font_scale, (0, 0, 0), thickness)\n",
    "        x = width * idx + (width - cv2.getTextSize(bottom_label, font, font_scale, thickness)[0][0]) // 2\n",
    "        cv2.putText(bottom_label_bg, bottom_label, (x, 20), font, font_scale, (0, 0, 0), thickness)\n",
    "\n",
    "    # Process GradCAM results\n",
    "    def visualize_gradcam(cam_output, original_image):\n",
    "        cam_output[cam_output < 0.7] = 0\n",
    "        img_gray = cv2.cvtColor(original_image, cv2.COLOR_RGB2GRAY) \n",
    "        img_gray = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB)\n",
    "        heatmap = np.zeros((cam_output.shape[0], cam_output.shape[1], 3), dtype=np.uint8)\n",
    "        heatmap[..., 0] = np.uint8(255 * cam_output)\n",
    "        return cv2.addWeighted(img_gray, 0.7, heatmap, 0.3, 0)\n",
    "\n",
    "    # Create visualization rows\n",
    "    top_row = np.concatenate([\n",
    "        orig_img,\n",
    "        visualize_gradcam(methods_results['GradCAM']['nmc'], orig_img.copy()),\n",
    "        visualize_gradcam(methods_results['GradCAM']['aptos'], orig_img.copy()),\n",
    "        visualize_gradcam(methods_results['GradCAM']['ft_aptos'], orig_img.copy())\n",
    "    ], axis=1)\n",
    "\n",
    "    bottom_row = np.concatenate([\n",
    "        label_img,\n",
    "        np.uint8(255 * methods_results['Integrated Gradients']['nmc']),\n",
    "        np.uint8(255 * methods_results['Integrated Gradients']['aptos']),\n",
    "        np.uint8(255 * methods_results['Integrated Gradients']['ft_aptos'])\n",
    "    ], axis=1)\n",
    "\n",
    "    # Combine all elements\n",
    "    final_image = np.vstack([\n",
    "        info_bg,\n",
    "        top_label_bg,\n",
    "        top_row,\n",
    "        bottom_label_bg,\n",
    "        bottom_row\n",
    "    ])\n",
    "\n",
    "    # Save the final image\n",
    "    cv2.imwrite(save_path, cv2.cvtColor(final_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "def find_label_image(base_name, label_dir='nmc_labeling'):\n",
    "    \"\"\"Find label image that contains the base image name\"\"\"\n",
    "    try:\n",
    "        for filename in os.listdir(label_dir):\n",
    "            if base_name in filename:\n",
    "                return os.path.join(label_dir, filename)\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def compare_and_save_visualizations(nmc_model, aptos_model, ft_aptos_model, dataloader, nmc_label_idx, aptos_label_idx, device, save_dir='visualization_results'):\n",
    "    categories = {\n",
    "        'both_correct': [], 'only_nmc_correct': [], \n",
    "        'only_aptos_correct': [], 'both_wrong': []\n",
    "    }\n",
    "    \n",
    "    for model in [nmc_model, aptos_model, ft_aptos_model]:\n",
    "        model.eval()\n",
    "        \n",
    "    save_path = f\"{save_dir}/comparison/label_{'-'.join(map(str, nmc_label_idx))}_vs_{aptos_label_idx}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    for batch_idx, (images, labels, img_name) in enumerate(dataloader):\n",
    "        if labels.item() != aptos_label_idx:\n",
    "            continue\n",
    "            \n",
    "        images = images.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get model outputs\n",
    "            nmc_outputs = nmc_model(images)\n",
    "            aptos_outputs = aptos_model(images)\n",
    "            ft_aptos_outputs = ft_aptos_model(images)\n",
    "            \n",
    "            # Single label case handling\n",
    "            if len(nmc_label_idx) == 1:\n",
    "                nmc_preds = (torch.sigmoid(nmc_outputs) > 0.5).squeeze()\n",
    "                nmc_raw = torch.sigmoid(nmc_outputs).squeeze()\n",
    "                if len(images) == 1:\n",
    "                    nmc_preds = nmc_preds.unsqueeze(0)\n",
    "                    nmc_raw = nmc_raw.unsqueeze(0)\n",
    "            else:\n",
    "                # Multi-label case\n",
    "                nmc_preds = (torch.sigmoid(nmc_outputs) > 0.5)\n",
    "                nmc_raw = torch.sigmoid(nmc_outputs)\n",
    "            \n",
    "            # Handle APTOS and FT-APTOS predictions\n",
    "            aptos_preds = (torch.sigmoid(aptos_outputs) > 0.5).squeeze()\n",
    "            ft_aptos_preds = (torch.sigmoid(ft_aptos_outputs) > 0.5).squeeze()\n",
    "            \n",
    "            if len(images) == 1:\n",
    "                aptos_preds = aptos_preds.unsqueeze(0)\n",
    "                ft_aptos_preds = ft_aptos_preds.unsqueeze(0)\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            try:\n",
    "                # Format predictions\n",
    "                if len(nmc_label_idx) > 1:\n",
    "                    nmc_pred_list = nmc_preds[i].cpu().numpy()\n",
    "                    nmc_pred_str = f\"({','.join(map(str, nmc_pred_list.astype(int)))})\"\n",
    "                    is_nmc_correct = nmc_pred_list.all()\n",
    "                else:\n",
    "                    nmc_pred_val = int(nmc_preds[i].item())\n",
    "                    nmc_pred_str = str(nmc_pred_val)\n",
    "                    is_nmc_correct = bool(nmc_pred_val)\n",
    "                \n",
    "                is_aptos_correct = bool(aptos_preds[i].item())\n",
    "                is_ft_aptos_correct = bool(ft_aptos_preds[i].item())\n",
    "                \n",
    "                methods_results = {\n",
    "                    'GradCAM': {\n",
    "                        'nmc': get_gradcam(nmc_model, images[i], nmc_label_idx, device),\n",
    "                        'aptos': get_gradcam(aptos_model, images[i], [0], device),\n",
    "                        'ft_aptos': get_gradcam(ft_aptos_model, images[i], [0], device)\n",
    "                    },\n",
    "                    'Integrated Gradients': {\n",
    "                        'nmc': get_integrated_gradients(nmc_model, images[i], nmc_label_idx, device),\n",
    "                        'aptos': get_integrated_gradients(aptos_model, images[i], [0], device),\n",
    "                        'ft_aptos': get_integrated_gradients(ft_aptos_model, images[i], [0], device)\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                category = 'both_correct' if is_nmc_correct and is_aptos_correct else \\\n",
    "                          'only_nmc_correct' if is_nmc_correct and not is_aptos_correct else \\\n",
    "                          'only_aptos_correct' if not is_nmc_correct and is_aptos_correct else \\\n",
    "                          'both_wrong'\n",
    "                \n",
    "                if len(categories[category]) < 3:\n",
    "                    sample_info = {\n",
    "                        'image': images[i].cpu().numpy(),\n",
    "                        'methods_results': methods_results,\n",
    "                        'label_info': [\n",
    "                            f\"GT: 1    NMC: {nmc_pred_str}    APTOS: {int(aptos_preds[i].item())}    FT-APTOS: {int(ft_aptos_preds[i].item())}\",\n",
    "                            f\"Image: {img_name[i]}\"  # Keep image name but don't display it\n",
    "                        ],\n",
    "                        'is_correct': {\n",
    "                            'nmc': is_nmc_correct,\n",
    "                            'aptos': is_aptos_correct,\n",
    "                            'ft_aptos': is_ft_aptos_correct\n",
    "                        }\n",
    "    }\n",
    "                    categories[category].append(sample_info)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i} in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        if all(len(cat) >= 3 for cat in categories.values()):\n",
    "            break\n",
    "    \n",
    "    for category_name, samples in categories.items():\n",
    "        for idx, sample in enumerate(samples):\n",
    "            try:\n",
    "                save_name = os.path.join(save_path, f'{category_name}_{idx}.png')\n",
    "                create_visualization_comparison(\n",
    "                    sample['image'],\n",
    "                    sample['methods_results'],\n",
    "                    sample['label_info'],\n",
    "                    sample['is_correct'],\n",
    "                    save_name\n",
    "                )\n",
    "                print(f\"Saved visualization: {save_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating visualization for {category_name} sample {idx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = [0, 255, 0]\n",
    "R = [255, 0, 0]\n",
    "\n",
    "def convert_to_gray_scale(attributions):\n",
    "    return np.average(attributions, axis=2)\n",
    "\n",
    "def linear_transform(attributions, clip_above_percentile=99.9, clip_below_percentile=70.0, low=0.2, plot_distribution=False):\n",
    "    m = compute_threshold_by_top_percentage(attributions, percentage=100-clip_above_percentile, plot_distribution=plot_distribution)\n",
    "    e = compute_threshold_by_top_percentage(attributions, percentage=100-clip_below_percentile, plot_distribution=plot_distribution)\n",
    "    transformed = (1 - low) * (np.abs(attributions) - e) / (m - e) + low\n",
    "    transformed *= np.sign(attributions)\n",
    "    transformed *= (transformed >= low)\n",
    "    transformed = np.clip(transformed, 0.0, 1.0)\n",
    "    return transformed\n",
    "\n",
    "def compute_threshold_by_top_percentage(attributions, percentage=60, plot_distribution=True):\n",
    "    if percentage < 0 or percentage > 100:\n",
    "        raise ValueError('percentage must be in [0, 100]')\n",
    "    if percentage == 100:\n",
    "        return np.min(attributions)\n",
    "    flat_attributions = attributions.flatten()\n",
    "    attribution_sum = np.sum(flat_attributions)\n",
    "    sorted_attributions = np.sort(np.abs(flat_attributions))[::-1]\n",
    "    cum_sum = 100.0 * np.cumsum(sorted_attributions) / attribution_sum\n",
    "    threshold_idx = np.where(cum_sum >= percentage)[0][0]\n",
    "    threshold = sorted_attributions[threshold_idx]\n",
    "    if plot_distribution:\n",
    "        raise NotImplementedError \n",
    "    return threshold\n",
    "\n",
    "def polarity_function(attributions, polarity):\n",
    "    if polarity == 'positive':\n",
    "        return np.clip(attributions, 0, 1)\n",
    "    elif polarity == 'negative':\n",
    "        return np.clip(attributions, -1, 0)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def overlay_function(attributions, image):\n",
    "    return np.clip(0.7 * image + 0.5 * attributions, 0, 255)\n",
    "\n",
    "def visualize(attributions, image, positive_channel=G, negative_channel=R, polarity='positive', \\\n",
    "                clip_above_percentile=99.9, clip_below_percentile=0, morphological_cleanup=False, \\\n",
    "                structure=np.ones((3, 3)), outlines=False, outlines_component_percentage=90, overlay=True, \\\n",
    "                mask_mode=False, plot_distribution=False):\n",
    "    if polarity == 'both':\n",
    "        raise NotImplementedError\n",
    "\n",
    "    elif polarity == 'positive':\n",
    "        attributions = polarity_function(attributions, polarity=polarity)\n",
    "        channel = positive_channel\n",
    "    \n",
    "    # convert the attributions to the gray scale\n",
    "    attributions = convert_to_gray_scale(attributions)\n",
    "    attributions = linear_transform(attributions, clip_above_percentile, clip_below_percentile, 0.0, plot_distribution=plot_distribution)\n",
    "    attributions_mask = attributions.copy()\n",
    "    if morphological_cleanup:\n",
    "        raise NotImplementedError\n",
    "    if outlines:\n",
    "        raise NotImplementedError\n",
    "    attributions = np.expand_dims(attributions, 2) * channel\n",
    "    if overlay:\n",
    "        if mask_mode == False:\n",
    "            attributions = overlay_function(attributions, image)\n",
    "        else:\n",
    "            attributions = np.expand_dims(attributions_mask, 2)\n",
    "            attributions = np.clip(attributions * image, 0, 255)\n",
    "            attributions = attributions[:, :, (2, 1, 0)]\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing NMC label [0] and APTOS label 0\n",
      "Saved visualization: visualization_results/comparison/label_0_vs_0/both_correct_0.png\n",
      "Saved visualization: visualization_results/comparison/label_0_vs_0/both_correct_1.png\n",
      "Saved visualization: visualization_results/comparison/label_0_vs_0/both_correct_2.png\n",
      "\n",
      "Processing NMC label [2] and APTOS label 1\n",
      "Saved visualization: visualization_results/comparison/label_2_vs_1/both_correct_0.png\n",
      "Saved visualization: visualization_results/comparison/label_2_vs_1/both_correct_1.png\n",
      "Saved visualization: visualization_results/comparison/label_2_vs_1/only_aptos_correct_0.png\n",
      "Saved visualization: visualization_results/comparison/label_2_vs_1/both_wrong_0.png\n",
      "\n",
      "Processing NMC label [1] and APTOS label 2\n",
      "Saved visualization: visualization_results/comparison/label_1_vs_2/both_correct_0.png\n",
      "Saved visualization: visualization_results/comparison/label_1_vs_2/both_correct_1.png\n",
      "Saved visualization: visualization_results/comparison/label_1_vs_2/only_aptos_correct_0.png\n",
      "\n",
      "Processing NMC label [1, 2] and APTOS label 3\n",
      "Saved visualization: visualization_results/comparison/label_1-2_vs_3/both_correct_0.png\n",
      "Saved visualization: visualization_results/comparison/label_1-2_vs_3/both_correct_1.png\n",
      "Saved visualization: visualization_results/comparison/label_1-2_vs_3/only_nmc_correct_0.png\n",
      "\n",
      "Processing NMC label [5, 6] and APTOS label 4\n",
      "Saved visualization: visualization_results/comparison/label_5-6_vs_4/only_aptos_correct_0.png\n",
      "Saved visualization: visualization_results/comparison/label_5-6_vs_4/only_aptos_correct_1.png\n",
      "Saved visualization: visualization_results/comparison/label_5-6_vs_4/both_wrong_0.png\n",
      "Saved visualization: visualization_results/comparison/label_5-6_vs_4/both_wrong_1.png\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "nmc_labels = [[0],[2],[1],[1,2],[5,6]]\n",
    "aptos_labels = [0,1,2,3,4]  # 각각 대응되는 APTOS 라벨\n",
    "\n",
    "for idx, nmc_label_idx in enumerate(nmc_labels):\n",
    "    aptos_label_idx = aptos_labels[idx]\n",
    "\n",
    "    print(f\"\\nProcessing NMC label {nmc_label_idx} and APTOS label {aptos_label_idx}\")\n",
    "\n",
    "    # Load NMC model\n",
    "    nmc_model = models.efficientnet_v2_m(pretrained=True)\n",
    "    num_ftrs = nmc_model.classifier[1].in_features\n",
    "    nmc_model.classifier = nn.Sequential(\n",
    "        nn.BatchNorm1d(num_ftrs),\n",
    "        nn.Linear(num_ftrs, len(nmc_label_idx))\n",
    "    )\n",
    "    nmc_model = nmc_model.to(device)\n",
    "\n",
    "    if len(nmc_label_idx)==1:\n",
    "        nmc_model.load_state_dict(torch.load(f'model/singlelabel/best_model_label_{nmc_label_idx[0]}_nmc_cnn.pth'))\n",
    "    else:\n",
    "        nmc_model.load_state_dict(torch.load(f'model/singlelabel/best_model_labels_{\"-\".join(map(str,nmc_label_idx))}_nmc_cnn.pth'))\n",
    "\n",
    "    # Load APTOS model\n",
    "    aptos_model = models.efficientnet_v2_m(pretrained=True)\n",
    "    aptos_model.classifier = nn.Sequential(\n",
    "        nn.BatchNorm1d(num_ftrs),\n",
    "        nn.Linear(num_ftrs, 1)\n",
    "    )\n",
    "    aptos_model = aptos_model.to(device)\n",
    "    aptos_model.load_state_dict(torch.load(f'model/singlelabel/best_model_label_{aptos_label_idx}_aptos_cnn.pth'))\n",
    "\n",
    "\n",
    "    # Load finetuned model\n",
    "    ft_aptos_model = models.efficientnet_v2_m(pretrained=True)\n",
    "    ft_aptos_model.classifier = nn.Sequential(\n",
    "        nn.BatchNorm1d(num_ftrs),\n",
    "        nn.Linear(num_ftrs, 1)\n",
    "    )\n",
    "    ft_aptos_model = ft_aptos_model.to(device)\n",
    "    ft_aptos_model.load_state_dict(torch.load(f'model/singlelabel_finetuning/best_model_label_{aptos_label_idx}_aptos_cnn.pth'))\n",
    "   \n",
    "    # Compare and save visualizations with all three models\n",
    "    compare_and_save_visualizations(\n",
    "        nmc_model, \n",
    "        aptos_model,\n",
    "        ft_aptos_model,\n",
    "        testloader, \n",
    "        nmc_label_idx, \n",
    "        aptos_label_idx, \n",
    "        device,\n",
    "        save_dir='visualization_results'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
