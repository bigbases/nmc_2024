{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch \n",
        "import argparse\n",
        "import yaml\n",
        "import time\n",
        "import multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "from tabulate import tabulate\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "#from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DistributedSampler, RandomSampler\n",
        "import sys\n",
        "sys.path.append('/workspace/nmc_2024/')\n",
        "from torch import distributed as dist\n",
        "from nmc.models import *\n",
        "from nmc.datasets import * \n",
        "from nmc.augmentations import get_train_augmentation, get_val_augmentation\n",
        "from nmc.losses import get_loss\n",
        "from nmc.schedulers import get_scheduler\n",
        "from nmc.optimizers import get_optimizer\n",
        "from nmc.utils.utils import fix_seeds, setup_cudnn, cleanup_ddp, setup_ddp\n",
        "from tools.val import evaluate_epi\n",
        "from nmc.utils.episodic_utils import * \n",
        "from scipy.cluster import hierarchy\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mutual_info_score\n",
        "from scipy.cluster import hierarchy\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torch.utils.data import Subset\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('../configs/NMC.yaml') as f:\n",
        "    cfg = yaml.load(f, Loader=yaml.SafeLoader)\n",
        "print(cfg)\n",
        "fix_seeds(3407)\n",
        "setup_cudnn()\n",
        "gpu = setup_ddp()\n",
        "save_dir = Path(cfg['SAVE_DIR'])\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "cleanup_ddp()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early Stopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_score):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_score\n",
        "        elif val_score < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = val_score\n",
        "            self.counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_train_augmentation(size):\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.Lambda(lambda x: x.float() if x.dtype == torch.uint8 else x),\n",
        "        transforms.Lambda(lambda x: x / 255.0 if x.max() > 1.0 else x),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "def get_val_test_transform(size):\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.Lambda(lambda x: x.float() if x.dtype == torch.uint8 else x),\n",
        "        transforms.Lambda(lambda x: x / 255.0 if x.max() > 1.0 else x),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BalancedBatchSampler(Sampler):\n",
        "    def __init__(self, dataset, batch_size):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # 데이터셋에서 레이블 추출\n",
        "        if hasattr(dataset, 'labels'):\n",
        "            self.labels = dataset.labels\n",
        "            if isinstance(self.labels, np.ndarray):\n",
        "                self.labels = torch.from_numpy(self.labels)\n",
        "        elif hasattr(dataset, 'targets'):\n",
        "            self.labels = dataset.targets\n",
        "            if isinstance(self.labels, np.ndarray):\n",
        "                self.labels = torch.from_numpy(self.labels)\n",
        "        else:\n",
        "            try:\n",
        "                self.labels = [sample[1] for sample in dataset]\n",
        "                if isinstance(self.labels[0], np.ndarray):\n",
        "                    self.labels = torch.from_numpy(np.array(self.labels))\n",
        "                else:\n",
        "                    self.labels = torch.tensor(self.labels)\n",
        "            except:\n",
        "                raise ValueError(\"Cannot access labels from dataset\")\n",
        "        \n",
        "        self.n_classes = self.labels.shape[1] if len(self.labels.shape) > 1 else len(torch.unique(self.labels))\n",
        "        self.samples_per_class = batch_size // self.n_classes\n",
        "        \n",
        "        # 클래스별 인덱스 저장\n",
        "        self.class_indices = []\n",
        "        for i in range(self.n_classes):\n",
        "            if len(self.labels.shape) > 1:\n",
        "                idx = torch.where(self.labels[:, i] == 1)[0]\n",
        "            else:\n",
        "                idx = torch.where(self.labels == i)[0]\n",
        "            self.class_indices.append(idx)\n",
        "        \n",
        "        self.n_batches = len(self.dataset) // batch_size\n",
        "        if len(self.dataset) % batch_size != 0:\n",
        "            self.n_batches += 1\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for _ in range(self.n_batches):\n",
        "            batch_indices = []\n",
        "            for class_idx in range(self.n_classes):\n",
        "                class_samples = self.class_indices[class_idx]\n",
        "                if len(class_samples) == 0:\n",
        "                    continue\n",
        "                \n",
        "                # 랜덤 선택\n",
        "                selected = class_samples[torch.randint(len(class_samples), \n",
        "                                                     (self.samples_per_class,))]\n",
        "                batch_indices.extend(selected.tolist())\n",
        "            \n",
        "            # 배치 크기에 맞게 자르기\n",
        "            if len(batch_indices) > self.batch_size:\n",
        "                batch_indices = batch_indices[:self.batch_size]\n",
        "            \n",
        "            # 중요: 리스트로 yield\n",
        "            yield batch_indices\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "best_mf1 = 0.0\n",
        "device = torch.device('cuda:1')\n",
        "print(\"device : \", device)\n",
        "num_workers = mp.cpu_count()\n",
        "train_cfg, eval_cfg = cfg['TRAIN'], cfg['EVAL']\n",
        "dataset_cfg, model_cfg = cfg['DATASET'], cfg['MODEL']\n",
        "dataset_cfg['ROOT'] = '/datas/fundus_dataset/NMC'\n",
        "loss_cfg, optim_cfg, sched_cfg = cfg['LOSS'], cfg['OPTIMIZER'], cfg['SCHEDULER']\n",
        "epochs, lr = train_cfg['EPOCHS'], optim_cfg['LR']\n",
        "\n",
        "image_size = [256,256]\n",
        "image_dir = Path(dataset_cfg['ROOT']) / 'train_images'\n",
        "train_transform = get_train_augmentation(image_size)\n",
        "val_test_transform = get_val_test_transform(image_size)\n",
        "batch_size = 32\n",
        "\n",
        "dataset = eval(dataset_cfg['NAME'])(\n",
        "    dataset_cfg['ROOT'] + '/cropped_images',\n",
        "    dataset_cfg['TRAIN_RATIO'],\n",
        "    dataset_cfg['VALID_RATIO'],\n",
        "    dataset_cfg['TEST_RATIO'],\n",
        "    transform=None\n",
        ")\n",
        "trainset, valset, testset = dataset.get_splits()\n",
        "trainset.transform = train_transform\n",
        "valset.transform = val_test_transform\n",
        "testset.transform = val_test_transform\n",
        "\n",
        "\n",
        "\n",
        "# DataLoader 수정\n",
        "trainloader = DataLoader(\n",
        "    trainset, \n",
        "    batch_sampler=BalancedBatchSampler(trainset, batch_size=batch_size),\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "# trainloader = DataLoader(trainset, batch_size=batch_size, num_workers=num_workers, drop_last=True, pin_memory=True)\n",
        "valloader = DataLoader(valset, batch_size=1, num_workers=1, pin_memory=True)\n",
        "testloader = DataLoader(testset, batch_size=1, num_workers=1, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_all_labels(dataloader):\n",
        "    all_labels = []\n",
        "    for _, labels in dataloader:\n",
        "        all_labels.append(labels.numpy())\n",
        "    return np.vstack(all_labels)\n",
        "\n",
        "def plot_multilabel_confusion_matrix(labels, class_names):\n",
        "    n_classes = len(class_names)\n",
        "    \n",
        "    # 전체 폰트 크기 설정\n",
        "    plt.rcParams.update({'font.size': 14})  # 기본 폰트 크기 14로 설정\n",
        "    \n",
        "    # 레이블 분포 시각화를 위한 confusion matrix 계산\n",
        "    # 각 클래스별로 다른 클래스와의 동시 발생 빈도를 계산\n",
        "    cooccurrence_matrix = np.zeros((n_classes, n_classes))\n",
        "    \n",
        "    for i in range(len(labels)):\n",
        "        for j in range(n_classes):\n",
        "            for k in range(n_classes):\n",
        "                if labels[i][j] == 1 and labels[i][k] == 1:\n",
        "                    cooccurrence_matrix[j][k] += 1\n",
        "    \n",
        "    # 시각화\n",
        "    plt.figure(figsize=(14, 12))  # 그림 크기도 좀 더 크게\n",
        "    sns.heatmap(cooccurrence_matrix, \n",
        "                annot=True, \n",
        "                fmt='g',\n",
        "                cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names,\n",
        "                annot_kws={'size': 22},  # 히트맵 내부 숫자 폰트 크기\n",
        "                cbar_kws={\"shrink\": 0.8})  # 컬러바 크기 조정\n",
        "    \n",
        "    plt.title('Label Co-occurrence Matrix', fontsize=26)  # 제목 폰트 크기\n",
        "    plt.xlabel('Labels', fontsize=24)  # x축 라벨 폰트 크기\n",
        "    plt.ylabel('Labels', fontsize=24)  # y축 라벨 폰트 크기\n",
        "    plt.xticks(fontsize=22)  # x축 틱 폰트 크기\n",
        "    plt.yticks(fontsize=22)  # y축 틱 폰트 크기\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 각 클래스별 통계 출력\n",
        "    print(\"\\nLabel Statistics:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        count = np.sum(labels[:, i])\n",
        "        percentage = (count / len(labels)) * 100\n",
        "        print(f\"{class_name}: {count} samples ({percentage:.2f}%)\")\n",
        "        \n",
        "    # 레이블 조합 분석\n",
        "    print(\"\\nCommon Label Combinations:\")\n",
        "    label_combinations = []\n",
        "    for label in labels:\n",
        "        combination = tuple(np.where(label == 1)[0])\n",
        "        label_combinations.append(combination)\n",
        "    \n",
        "    from collections import Counter\n",
        "    combination_counts = Counter(label_combinations)\n",
        "    for combination, count in combination_counts.most_common(5):\n",
        "        combination_names = [class_names[i] for i in combination]\n",
        "        percentage = (count / len(labels)) * 100\n",
        "        print(f\"{combination_names}: {count} samples ({percentage:.2f}%)\")\n",
        "\n",
        "# 모든 데이터셋의 레이블 통합\n",
        "def analyze_label_distribution(trainloader, valloader, testloader, class_names):\n",
        "    # 각 데이터셋의 레이블 수집\n",
        "    train_labels = get_all_labels(trainloader)\n",
        "    val_labels = get_all_labels(valloader)\n",
        "    test_labels = get_all_labels(testloader)\n",
        "    \n",
        "    # 모든 레이블 통합\n",
        "    combined_labels = np.vstack([train_labels, val_labels, test_labels])\n",
        "    \n",
        "    # 데이터셋별 분포 출력\n",
        "    print(\"Dataset Sizes:\")\n",
        "    print(f\"Train: {len(train_labels)} samples\")\n",
        "    print(f\"Validation: {len(val_labels)} samples\")\n",
        "    print(f\"Test: {len(test_labels)} samples\")\n",
        "    print(f\"Total: {len(combined_labels)} samples\\n\")\n",
        "    \n",
        "    # 통합된 레이블에 대한 분석 수행\n",
        "    plot_multilabel_confusion_matrix(combined_labels, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_names = ['class_'+str(i) for i in range(7)]  # 실제 클래스 이름으로 대체\n",
        "analyze_label_distribution( trainloader, valloader, testloader, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Read the labels\n",
        "df = pd.read_csv('/datas/fundus_dataset/NMC/final_label.csv')\n",
        "\n",
        "# Convert string labels to list and check for classes 9 or 10\n",
        "def convert_label(x):\n",
        "    has_invalid_class = False\n",
        "    result_list = []\n",
        "    \n",
        "    if isinstance(x, str):\n",
        "        # Remove brackets and split by comma\n",
        "        items = str(x).strip('[]').split(',')\n",
        "        # Check each item\n",
        "        for item in items:\n",
        "            if item.strip():\n",
        "                val = int(item.strip())\n",
        "                # if val in [9, 10]:  # 클래스 9나 10이 포함되어 있으면 플래그 설정\n",
        "                #     has_invalid_class = True\n",
        "                # elif val < 7:  # 유효한 클래스(0-6)만 추가\n",
        "                result_list.append(val)\n",
        "    elif isinstance(x, (int, float)):\n",
        "        val = int(x)\n",
        "        # if val in [7, 8, 9, 10]:\n",
        "        #     has_invalid_class = True\n",
        "        # elif val < 7:\n",
        "        result_list.append(val)\n",
        "    \n",
        "    # 클래스 9나 10이 포함된 경우 None 반환 (이후에 필터링하기 위함)\n",
        "    return None if has_invalid_class else result_list\n",
        "\n",
        "# 라벨 변환 적용\n",
        "df['label'] = df['label'].apply(convert_label)\n",
        "\n",
        "# 클래스 9나 10이 포함된 행(None으로 변환된 행) 제거\n",
        "df_filtered = df.dropna(subset=['label'])\n",
        "print(f\"원본 데이터 수: {len(df)}\")\n",
        "print(f\"필터링 후 데이터 수: {len(df_filtered)}\")\n",
        "\n",
        "# 클래스 0-6만 사용\n",
        "valid_classes = list(range(7))  # [0, 1, 2, 3, 4, 5, 6]\n",
        "mlb = MultiLabelBinarizer(classes=valid_classes)\n",
        "y_true_binary = mlb.fit_transform(df_filtered['label'])\n",
        "\n",
        "# 변환 결과 확인\n",
        "print(f\"Shape of y_true_binary: {y_true_binary.shape}\")\n",
        "print(f\"Sum of each class: {y_true_binary.sum(axis=0)}\")\n",
        "print(f\"Classes: {mlb.classes_}\")\n",
        "\n",
        "# Calculate co-occurrence matrix\n",
        "true_cooccurrence = np.zeros((len(mlb.classes_), len(mlb.classes_)))\n",
        "for row in y_true_binary:\n",
        "    # Find indices where labels are present (1)\n",
        "    present_labels = np.where(row == 1)[0]\n",
        "    # Update co-occurrence for all pairs of present labels\n",
        "    for i in present_labels:\n",
        "        for j in present_labels:\n",
        "            true_cooccurrence[i][j] += 1\n",
        "\n",
        "# 시각화를 위한 라벨 이름 생성\n",
        "label_names = ['class_'+str(i) for i in mlb.classes_]\n",
        "\n",
        "# Create heatmap for true labels co-occurrence\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(true_cooccurrence, annot=True, fmt='.0f', cmap='Blues',\n",
        "            xticklabels=label_names, yticklabels=label_names)\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Label')\n",
        "plt.title('Label Co-occurrence Matrix (Classes 0-6, excluding rows with classes 9 or 10)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nmc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}