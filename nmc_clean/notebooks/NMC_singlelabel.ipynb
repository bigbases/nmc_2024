{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 싱글모델로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import argparse\n",
    "import yaml\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DistributedSampler, RandomSampler\n",
    "from torch import distributed as dist\n",
    "from nmc.models import *\n",
    "from nmc.datasets import * \n",
    "from nmc.augmentations import get_train_augmentation, get_val_augmentation\n",
    "from nmc.losses import get_loss\n",
    "from nmc.schedulers import get_scheduler\n",
    "from nmc.optimizers import get_optimizer\n",
    "from nmc.utils.utils import fix_seeds, setup_cudnn, cleanup_ddp, setup_ddp\n",
    "from tools.val import evaluate_epi\n",
    "from nmc.utils.episodic_utils import * \n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.cluster import hierarchy\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DEVICE': 'cuda:0', 'SAVE_DIR': 'output', 'MODEL': {'NAME': 'EfficientNetV2MModelMulti', 'BACKBONE': 'EfficientNetV2', 'PRETRAINED': '/workspace/jhmoon/nmc_2024/checkpoints/pretrained/tf_efficientnetv2_m_weights.pth', 'UNFREEZE': 'full', 'VERSION': \"384_32_loss'\"}, 'DATASET': {'NAME': 'NMCDataset', 'ROOT': '/datas/fundus_dataset/NMC', 'TRAIN_RATIO': 0.7, 'VALID_RATIO': 0.15, 'TEST_RATIO': 0.15}, 'TRAIN': {'IMAGE_SIZE': [384, 384], 'BATCH_SIZE': 32, 'EPOCHS': 100, 'EVAL_INTERVAL': 1, 'AMP': False, 'DDP': False}, 'LOSS': {'NAME': 'BCEWithLogitsLoss', 'CLS_WEIGHTS': False}, 'OPTIMIZER': {'NAME': 'adamw', 'LR': 0.1, 'WEIGHT_DECAY': 0.01}, 'SCHEDULER': {'NAME': 'warmuppolylr', 'POWER': 0.9, 'WARMUP': 10, 'WARMUP_RATIO': 0.1}, 'EVAL': {'MODEL_PATH': 'checkpoints/pretrained/FGMaxxVit/FGMaxxVit.FGMaxxVit.NMC.pth', 'IMAGE_SIZE': [384, 384]}, 'TEST': {'MODEL_PATH': 'checkpoints/pretrained/FGMaxxVit/FGMaxxVit.FGMaxxVit.NMC.pth', 'FILE': 'assests/ade', 'IMAGE_SIZE': [384, 384], 'OVERLAY': True}}\n"
     ]
    }
   ],
   "source": [
    "with open('../configs/NMC.yaml') as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "print(cfg)\n",
    "fix_seeds(3407)\n",
    "setup_cudnn()\n",
    "gpu = setup_ddp()\n",
    "save_dir = Path(cfg['SAVE_DIR'])\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "cleanup_ddp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_augmentation(size):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.Lambda(lambda x: x.float() if x.dtype == torch.uint8 else x),\n",
    "        transforms.Lambda(lambda x: x / 255.0 if x.max() > 1.0 else x),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_val_test_transform(size):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.Lambda(lambda x: x.float() if x.dtype == torch.uint8 else x),\n",
    "        transforms.Lambda(lambda x: x / 255.0 if x.max() > 1.0 else x),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTargetBalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, target_classes):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.target_classes = target_classes\n",
    "        \n",
    "        # 데이터셋에서 레이블 추출\n",
    "        if hasattr(dataset, 'labels'):\n",
    "            self.labels = dataset.labels\n",
    "            if isinstance(self.labels, np.ndarray):\n",
    "                self.labels = torch.from_numpy(self.labels)\n",
    "        elif hasattr(dataset, 'targets'):\n",
    "            self.labels = dataset.targets\n",
    "            if isinstance(self.labels, np.ndarray):\n",
    "                self.labels = torch.from_numpy(self.labels)\n",
    "        else:\n",
    "            try:\n",
    "                self.labels = [sample[1] for sample in dataset]\n",
    "                if isinstance(self.labels[0], np.ndarray):\n",
    "                    self.labels = torch.from_numpy(np.array(self.labels))\n",
    "                else:\n",
    "                    self.labels = torch.tensor(self.labels)\n",
    "            except:\n",
    "                raise ValueError(\"Cannot access labels from dataset\")\n",
    "        \n",
    "        # 각 타겟 클래스와 나머지 클래스의 인덱스 저장\n",
    "        self.target_indices = {}\n",
    "        for target in target_classes:\n",
    "            if len(self.labels.shape) > 1:\n",
    "                self.target_indices[target] = torch.where(self.labels[:, target] == 1)[0]\n",
    "            else:\n",
    "                self.target_indices[target] = torch.where(self.labels == target)[0]\n",
    "        \n",
    "        # 나머지 클래스의 인덱스 저장\n",
    "        if len(self.labels.shape) > 1:\n",
    "            self.other_indices = torch.where(\n",
    "                torch.sum(self.labels[:, target_classes], dim=1) == 0)[0]\n",
    "        else:\n",
    "            mask = torch.ones_like(self.labels, dtype=torch.bool)\n",
    "            for target in target_classes:\n",
    "                mask &= (self.labels != target)\n",
    "            self.other_indices = torch.where(mask)[0]\n",
    "        \n",
    "        # 각 그룹당 샘플 수 계산\n",
    "        n_groups = len(target_classes) + 1  # 타겟 클래스들 + 나머지\n",
    "        self.samples_per_group = batch_size // n_groups\n",
    "        \n",
    "        self.n_batches = len(self.dataset) // batch_size\n",
    "        if len(self.dataset) % batch_size != 0:\n",
    "            self.n_batches += 1\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for _ in range(self.n_batches):\n",
    "            batch_indices = []\n",
    "            \n",
    "            # 각 타겟 클래스에서 샘플링\n",
    "            for target in self.target_classes:\n",
    "                target_selected = self.target_indices[target][\n",
    "                    torch.randint(len(self.target_indices[target]), \n",
    "                                (self.samples_per_group,))\n",
    "                ]\n",
    "                batch_indices.extend(target_selected.tolist())\n",
    "            \n",
    "            # 나머지 클래스들에서 샘플링\n",
    "            other_selected = self.other_indices[\n",
    "                torch.randint(len(self.other_indices), \n",
    "                            (self.samples_per_group,))\n",
    "            ]\n",
    "            batch_indices.extend(other_selected.tolist())\n",
    "            \n",
    "            # 배치 셔플\n",
    "            random.shuffle(batch_indices)\n",
    "            \n",
    "            # 배치 크기에 맞게 자르기 (나누어 떨어지지 않는 경우 처리)\n",
    "            if len(batch_indices) > self.batch_size:\n",
    "                batch_indices = batch_indices[:self.batch_size]\n",
    "            \n",
    "            yield batch_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, scaler, device, target_label_idx):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_targets = len(target_label_idx)\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        if num_targets == 1:\n",
    "            # 단일 레이블 케이스\n",
    "            target_labels = labels[:, target_label_idx].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast(enabled=True):\n",
    "                outputs = model(images)\n",
    "                # 차원을 맞춰줌\n",
    "                outputs = outputs.view(-1)  # or outputs.squeeze()\n",
    "                target_labels = target_labels.view(-1)  # or target_labels.squeeze()\n",
    "                loss = criterion(outputs, target_labels)\n",
    "        else:\n",
    "            # 다중 레이블 케이스\n",
    "            target_labels = labels[:, target_label_idx].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast(enabled=True):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, target_labels)\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, target_label_idx):\n",
    "   model.eval()\n",
    "   all_preds = []\n",
    "   all_labels = []\n",
    "   num_targets = len(target_label_idx)\n",
    "   \n",
    "   with torch.no_grad():\n",
    "       for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "           images = images.to(device)\n",
    "           \n",
    "           if num_targets == 1:\n",
    "               # 단일 레이블 케이스\n",
    "               target_labels = labels[:, target_label_idx].to(device)\n",
    "               outputs = model(images)\n",
    "               \n",
    "               # 차원 처리\n",
    "               if len(outputs.shape) == 2:\n",
    "                   outputs = outputs.squeeze(1)\n",
    "               \n",
    "               preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "               \n",
    "               all_preds.append(preds.cpu().numpy().reshape(-1))\n",
    "               all_labels.append(target_labels.cpu().numpy().reshape(-1))\n",
    "           \n",
    "           else:\n",
    "               # 다중 레이블 케이스\n",
    "               target_labels = labels[:, target_label_idx].to(device)\n",
    "               outputs = model(images)\n",
    "               \n",
    "               # 각 레이블에 대한 예측\n",
    "               preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "               \n",
    "               all_preds.append(preds.cpu().numpy())\n",
    "               all_labels.append(target_labels.cpu().numpy())\n",
    "   \n",
    "   # numpy array로 변환\n",
    "   all_preds = np.concatenate(all_preds)\n",
    "   all_labels = np.concatenate(all_labels)\n",
    "   \n",
    "   if num_targets == 1:\n",
    "       # 단일 레이블 메트릭\n",
    "       f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "       accuracy = accuracy_score(all_labels, all_preds)\n",
    "       precision = precision_score(all_labels, all_preds)\n",
    "       recall = recall_score(all_labels, all_preds)\n",
    "       \n",
    "       return f1, accuracy, precision, recall\n",
    "   else:\n",
    "       # 다중 레이블 메트릭\n",
    "       f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "       accuracy = accuracy_score(all_labels, all_preds)\n",
    "       precision = precision_score(all_labels, all_preds, average='macro')\n",
    "       recall = recall_score(all_labels, all_preds, average='macro')\n",
    "       \n",
    "       return f1, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, scaler, device, epochs, target_label_idx):\n",
    "    best_f1 = 0.0\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "    num_targets = len(target_label_idx)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, scaler, device, target_label_idx)\n",
    "        \n",
    "        if num_targets == 1:\n",
    "            val_f1, val_acc, val_prec, val_rec = evaluate(model, val_loader, device, target_label_idx)\n",
    "            \n",
    "            print(f\"Training Loss: {train_loss:.4f}\")\n",
    "            print(f\"Validation Metrics:\")\n",
    "            print(f\"  F1 Score: {val_f1:.4f}\")\n",
    "            print(f\"  Accuracy: {val_acc:.4f}\") \n",
    "            print(f\"  Precision: {val_prec:.4f}\")\n",
    "            print(f\"  Recall: {val_rec:.4f}\")\n",
    "            \n",
    "            scheduler.step(val_f1)\n",
    "            \n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                torch.save(model.state_dict(), f'model/singlelabel/best_model_label_{target_label_idx[0]}_nmc_cnn.pth')\n",
    "                print(\"New best model saved!\")\n",
    "        else:\n",
    "            # 모든 메트릭을 받아서 f1만 사용\n",
    "            val_f1, val_acc, val_prec, val_rec = evaluate(model, val_loader, device, target_label_idx)\n",
    "            \n",
    "            print(f\"Training Loss: {train_loss:.4f}\")\n",
    "            print(f\"Validation Metrics:\")\n",
    "            print(f\"  Macro F1 Score: {val_f1:.4f}\")\n",
    "            print(f\"  Macro Accuracy: {val_acc:.4f}\")\n",
    "            print(f\"  Macro Precision: {val_prec:.4f}\")\n",
    "            print(f\"  Macro Recall: {val_rec:.4f}\")\n",
    "            \n",
    "            scheduler.step(val_f1)\n",
    "            \n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                torch.save(model.state_dict(), f'model/singlelabel/best_model_labels_{\"-\".join(map(str,target_label_idx))}_nmc_cnn.pth')\n",
    "                print(\"New best model saved!\")\n",
    "        \n",
    "        early_stopping(val_f1)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda:0\n",
      "/datas/fundus_dataset/NMC/cropped_images\n",
      "label\n",
      "(0,)               1935\n",
      "(3,)                538\n",
      "(1, 2, 3)           533\n",
      "(1, 2)              288\n",
      "(2,)                229\n",
      "(1, 2, 3, 4)        186\n",
      "(2, 3)              164\n",
      "(1,)                151\n",
      "(4,)                 40\n",
      "(1, 3)               32\n",
      "(1, 2, 4)            26\n",
      "(3, 4)               26\n",
      "(1, 4)               13\n",
      "(2, 4)               11\n",
      "(1, 2, 3, 4, 5)      11\n",
      "(5,)                  9\n",
      "(2, 3, 4)             9\n",
      "(1, 2, 3, 5)          8\n",
      "(1, 2, 5)             7\n",
      "(1, 2, 3, 5, 6)       5\n",
      "(1, 2, 3, 6)          4\n",
      "(1, 3, 4)             2\n",
      "(1, 3, 6)             1\n",
      "(6,)                  1\n",
      "(1, 2, 6)             1\n",
      "(1, 2, 3, 4, 6)       1\n",
      "Name: count, dtype: int64\n",
      "train size: 4231\n",
      "label\n",
      "(0,)            415\n",
      "(1, 2, 3)       112\n",
      "(3,)            112\n",
      "(1, 2)           69\n",
      "(1, 2, 3, 4)     45\n",
      "(2,)             43\n",
      "(2, 3)           39\n",
      "(1,)             26\n",
      "(1, 3)           10\n",
      "(4,)              9\n",
      "(3, 4)            5\n",
      "(5,)              5\n",
      "(2, 4)            5\n",
      "(1, 4)            4\n",
      "(1, 5)            3\n",
      "(2, 3, 4)         1\n",
      "(1, 2, 3, 6)      1\n",
      "(1, 2, 5)         1\n",
      "(1, 2, 4)         1\n",
      "(6,)              1\n",
      "Name: count, dtype: int64\n",
      "val size: 907\n",
      "label\n",
      "(0,)               415\n",
      "(3,)               118\n",
      "(1, 2, 3)          112\n",
      "(1, 2)              63\n",
      "(2,)                51\n",
      "(1, 2, 3, 4)        44\n",
      "(2, 3)              36\n",
      "(1,)                34\n",
      "(4,)                14\n",
      "(1, 3)               7\n",
      "(1, 2, 4)            6\n",
      "(5,)                 4\n",
      "(3, 4)               3\n",
      "(1, 2, 3, 5)         3\n",
      "(6,)                 2\n",
      "(1, 3, 4)            1\n",
      "(1, 2, 3, 4, 6)      1\n",
      "(2, 3, 4)            1\n",
      "(1, 2, 5)            1\n",
      "Name: count, dtype: int64\n",
      "test size: 916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/nmc/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/nmc/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_M_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 91\u001b[0m\n\u001b[1;32m     76\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# best_f1 = train_and_evaluate(\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#     efficientnet, \u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#     trainloader, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#     target_label_idx\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed. Best F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_f1' is not defined"
     ]
    }
   ],
   "source": [
    "ncm_aptos_labels = [[0],[2],[1],[1,2],[5,6]]\n",
    "for target_label_idx in ncm_aptos_labels:\n",
    "    if target_label_idx == [0] or target_label_idx == [2] or target_label_idx == [1]:\n",
    "        continue\n",
    "    start = time.time()\n",
    "    best_mf1 = 0.0\n",
    "    device = torch.device(cfg['DEVICE'])\n",
    "    print(\"device : \", device)\n",
    "    num_workers = mp.cpu_count()\n",
    "    train_cfg, eval_cfg = cfg['TRAIN'], cfg['EVAL']\n",
    "    dataset_cfg, model_cfg = cfg['DATASET'], cfg['MODEL']\n",
    "    loss_cfg, optim_cfg, sched_cfg = cfg['LOSS'], cfg['OPTIMIZER'], cfg['SCHEDULER']\n",
    "    epochs, lr = train_cfg['EPOCHS'], optim_cfg['LR']\n",
    "\n",
    "    image_size = [256,256]\n",
    "    image_dir = Path(dataset_cfg['ROOT']) / 'train_images'\n",
    "    train_transform = get_train_augmentation(image_size)\n",
    "    val_test_transform = get_val_test_transform(image_size)\n",
    "    batch_size = 32\n",
    "\n",
    "\n",
    "    dataset = eval(dataset_cfg['NAME'])(\n",
    "        dataset_cfg['ROOT'] + '/cropped_images',\n",
    "        dataset_cfg['TRAIN_RATIO'],\n",
    "        dataset_cfg['VALID_RATIO'],\n",
    "        dataset_cfg['TEST_RATIO'],\n",
    "        transform=None\n",
    "    )\n",
    "    trainset, valset, testset = dataset.get_splits()\n",
    "    trainset.transform = train_transform\n",
    "    valset.transform = val_test_transform\n",
    "    testset.transform = val_test_transform\n",
    "\n",
    "\n",
    "\n",
    "    # DataLoader 수정\n",
    "    trainloader = DataLoader(\n",
    "        trainset, \n",
    "        batch_sampler=MultiTargetBalancedBatchSampler(trainset, batch_size=batch_size, target_classes =target_label_idx),\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    # trainloader = DataLoader(trainset, batch_size=batch_size, num_workers=num_workers, drop_last=True, pin_memory=True)\n",
    "    valloader = DataLoader(valset, batch_size=1, num_workers=1, pin_memory=True)\n",
    "    testloader = DataLoader(testset, batch_size=1, num_workers=1, pin_memory=True)\n",
    "    \n",
    "    # Model definition (changed to binary classification)\n",
    "    efficientnet = models.efficientnet_v2_m(pretrained=True)\n",
    "    num_ftrs = efficientnet.classifier[1].in_features\n",
    "    num_targets = len(target_label_idx)\n",
    "    \n",
    "    \n",
    "    if num_targets == 1:\n",
    "        # 단일 레이블 케이스 (기존 코드와 동일)\n",
    "        efficientnet.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_ftrs),\n",
    "            nn.Linear(num_ftrs, 1)\n",
    "        )\n",
    "    else:\n",
    "        # 다중 레이블 케이스\n",
    "        efficientnet.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_ftrs),\n",
    "            nn.Linear(num_ftrs, num_targets)\n",
    "        )\n",
    "    efficientnet = efficientnet.to(device)\n",
    "    \n",
    "    # L2 regularization\n",
    "    weight_decay = 1e-4\n",
    "    optimizer = torch.optim.AdamW(efficientnet.parameters(), lr=0.0001, weight_decay=weight_decay)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = GradScaler(enabled=train_cfg['AMP'])\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    # Main execution code\n",
    "    epochs = 100\n",
    "\n",
    "\n",
    "    # best_f1 = train_and_evaluate(\n",
    "    #     efficientnet, \n",
    "    #     trainloader, \n",
    "    #     valloader, \n",
    "    #     criterion, \n",
    "    #     optimizer, \n",
    "    #     scaler, \n",
    "    #     device, \n",
    "    #     epochs,\n",
    "    #     target_label_idx\n",
    "    # )\n",
    "\n",
    "    print(f\"Training completed. Best F1 Score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, target_label_idx):\n",
    "   model.eval()\n",
    "   all_preds = []\n",
    "   all_labels = []\n",
    "   num_targets = len(target_label_idx)\n",
    "   \n",
    "   with torch.no_grad():\n",
    "       for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "           images = images.to(device)\n",
    "           \n",
    "           if num_targets == 1:\n",
    "               # 단일 레이블 케이스\n",
    "               target_labels = labels[:, target_label_idx].to(device)\n",
    "               outputs = model(images)\n",
    "               \n",
    "               # 차원 처리\n",
    "               if len(outputs.shape) == 2:\n",
    "                   outputs = outputs.squeeze(1)\n",
    "               \n",
    "               preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "               \n",
    "               all_preds.append(preds.cpu().numpy().reshape(-1))\n",
    "               all_labels.append(target_labels.cpu().numpy().reshape(-1))\n",
    "           \n",
    "           else:\n",
    "               # 다중 레이블 케이스\n",
    "               target_labels = labels[:, target_label_idx].to(device)\n",
    "               outputs = model(images)\n",
    "               \n",
    "               # 각 레이블에 대한 예측\n",
    "               preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "               \n",
    "               all_preds.append(preds.cpu().numpy())\n",
    "               all_labels.append(target_labels.cpu().numpy())\n",
    "   \n",
    "   # numpy array로 변환\n",
    "   all_preds = np.concatenate(all_preds)\n",
    "   all_labels = np.concatenate(all_labels)\n",
    "   \n",
    "   if num_targets == 1:\n",
    "       # 단일 레이블 메트릭\n",
    "       f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "       accuracy = accuracy_score(all_labels, all_preds)\n",
    "       precision = precision_score(all_labels, all_preds)\n",
    "       recall = recall_score(all_labels, all_preds)\n",
    "       \n",
    "       return f1, accuracy, precision, recall\n",
    "   else:\n",
    "       # 다중 레이블 메트릭\n",
    "       # 클래스별 F1 점수 계산\n",
    "       class_f1s = []\n",
    "       for i in range(num_targets):\n",
    "           class_f1 = f1_score(all_labels[:, i], all_preds[:, i], average='binary')\n",
    "           class_f1s.append(class_f1)\n",
    "           precision = precision_score(all_labels[:, i], all_preds[:, i])\n",
    "           recall = recall_score(all_labels[:, i], all_preds[:, i])\n",
    "           print(f\"Class {target_label_idx[i]} F1: {class_f1:.4f}\")\n",
    "           print(\"precision: \", precision)\n",
    "           print(\"recal: \",recall)       \n",
    "       # 전체 평균 메트릭\n",
    "       macro_f1 = np.mean(class_f1s)\n",
    "       accuracy = accuracy_score(all_labels, all_preds)\n",
    "       precision = precision_score(all_labels, all_preds, average='macro')\n",
    "       recall = recall_score(all_labels, all_preds, average='macro')\n",
    "       \n",
    "       return macro_f1, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 907/907 [00:40<00:00, 22.21it/s]\n",
      "/root/anaconda3/envs/gm/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/root/anaconda3/envs/gm/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_M_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "  F1 Score: 0.8921\n",
      "  Accuracy: 0.9008\n",
      "  Precision: 0.8878\n",
      "  Recall: 0.8964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 907/907 [00:42<00:00, 21.46it/s]\n",
      "/root/anaconda3/envs/gm/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/root/anaconda3/envs/gm/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_M_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "  F1 Score: 0.8644\n",
      "  Accuracy: 0.9052\n",
      "  Precision: 0.8671\n",
      "  Recall: 0.8616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 907/907 [00:42<00:00, 21.48it/s]\n",
      "/root/anaconda3/envs/gm/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/root/anaconda3/envs/gm/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_M_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "  F1 Score: 0.8535\n",
      "  Accuracy: 0.9107\n",
      "  Precision: 0.8399\n",
      "  Recall: 0.8676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 907/907 [00:41<00:00, 21.91it/s]\n",
      "/root/anaconda3/envs/gm/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/root/anaconda3/envs/gm/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_M_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 F1: 0.8491\n",
      "Class 2 F1: 0.8545\n",
      "Test Results:\n",
      "  F1 Score: 0.8518\n",
      "  Accuracy: 0.8379\n",
      "  Precision: 0.8613\n",
      "  Recall: 0.8428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 907/907 [00:42<00:00, 21.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 5 F1: 0.4000\n",
      "Class 6 F1: 0.0000\n",
      "Test Results:\n",
      "  F1 Score: 0.2000\n",
      "  Accuracy: 0.9901\n",
      "  Precision: 0.5000\n",
      "  Recall: 0.1250\n"
     ]
    }
   ],
   "source": [
    "ncm_aptos_labels = [[0],[2],[1],[1,2],[5,6]]\n",
    "for target_label_idx in ncm_aptos_labels:\n",
    "    # Model definition (changed to binary classification)\n",
    "    efficientnet = models.efficientnet_v2_m(pretrained=True)\n",
    "    num_ftrs = efficientnet.classifier[1].in_features\n",
    "    num_targets = len(target_label_idx)\n",
    "    \n",
    "    \n",
    "    if num_targets == 1:\n",
    "        # 단일 레이블 케이스 (기존 코드와 동일)\n",
    "        efficientnet.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_ftrs),\n",
    "            nn.Linear(num_ftrs, 1)\n",
    "        )\n",
    "    else:\n",
    "        # 다중 레이블 케이스\n",
    "        efficientnet.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_ftrs),\n",
    "            nn.Linear(num_ftrs, num_targets)\n",
    "        )\n",
    "    efficientnet = efficientnet.to(device)\n",
    "    \n",
    "    if len(target_label_idx)==1:\n",
    "        # Final evaluation on test set\n",
    "        efficientnet.load_state_dict(torch.load(f'model/singlelabel/best_model_label_{target_label_idx[0]}_nmc_cnn.pth'))\n",
    "    else:\n",
    "        efficientnet.load_state_dict(torch.load(f'model/singlelabel/best_model_labels_{\"-\".join(map(str,target_label_idx))}_nmc_cnn.pth'))\n",
    "    test_f1, test_acc, test_prec, test_rec = evaluate(efficientnet, testloader, device, target_label_idx)\n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Precision: {test_prec:.4f}\")\n",
    "    print(f\"  Recall: {test_rec:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 916/916 [00:30<00:00, 29.71it/s]\n",
      "/root/anaconda3/envs/nmc/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/nmc/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_M_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "  F1 Score: 0.9173\n",
      "  Accuracy: 0.9258\n",
      "  Precision: 0.9263\n",
      "  Recall: 0.9084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 916/916 [00:28<00:00, 31.89it/s]\n",
      "/root/anaconda3/envs/nmc/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/nmc/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_M_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "  F1 Score: 0.8858\n",
      "  Accuracy: 0.9203\n",
      "  Precision: 0.8816\n",
      "  Recall: 0.8899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  28%|██▊       | 261/916 [00:08<00:21, 31.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     efficientnet\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/singlelabel/best_model_labels_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,target_label_idx))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_nmc_cnn.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 28\u001b[0m test_f1, test_acc, test_prec, test_rec \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mefficientnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_label_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, device, target_label_idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_targets \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# 단일 레이블 케이스\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     target_labels \u001b[38;5;241m=\u001b[39m labels[:, target_label_idx]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# 차원 처리\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torchvision/models/efficientnet.py:355\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torchvision/models/efficientnet.py:345\u001b[0m, in \u001b[0;36mEfficientNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 345\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    348\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torchvision/models/efficientnet.py:165\u001b[0m, in \u001b[0;36mMBConv.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 165\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[1;32m    167\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstochastic_depth(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nmc/lib/python3.8/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ncm_aptos_labels = [[0],[2],[1],[1,2],[5,6]]\n",
    "for target_label_idx in ncm_aptos_labels:\n",
    "    # Model definition (changed to binary classification)\n",
    "    efficientnet = models.efficientnet_v2_m(pretrained=True)\n",
    "    num_ftrs = efficientnet.classifier[1].in_features\n",
    "    num_targets = len(target_label_idx)\n",
    "    \n",
    "    \n",
    "    if num_targets == 1:\n",
    "        # 단일 레이블 케이스 (기존 코드와 동일)\n",
    "        efficientnet.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_ftrs),\n",
    "            nn.Linear(num_ftrs, 1)\n",
    "        )\n",
    "    else:\n",
    "        # 다중 레이블 케이스\n",
    "        efficientnet.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_ftrs),\n",
    "            nn.Linear(num_ftrs, num_targets)\n",
    "        )\n",
    "    efficientnet = efficientnet.to(device)\n",
    "    \n",
    "    if len(target_label_idx)==1:\n",
    "        # Final evaluation on test set\n",
    "        efficientnet.load_state_dict(torch.load(f'model/singlelabel/best_model_label_{target_label_idx[0]}_nmc_cnn.pth'))\n",
    "    else:\n",
    "        efficientnet.load_state_dict(torch.load(f'model/singlelabel/best_model_labels_{\"-\".join(map(str,target_label_idx))}_nmc_cnn.pth'))\n",
    "    test_f1, test_acc, test_prec, test_rec = evaluate(efficientnet, testloader, device, target_label_idx)\n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Precision: {test_prec:.4f}\")\n",
    "    print(f\"  Recall: {test_rec:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
